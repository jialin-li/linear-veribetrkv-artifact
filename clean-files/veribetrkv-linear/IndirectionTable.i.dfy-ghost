// IndirectionTable.i.dfy
module IndirectionTable {
  datatype Entry = Entry(loc: Option<Location>, succs: seq<BT.G.Reference>, predCount: uint64)
  type HashMap = LinearMutableMap.LinearHashMap<Entry>
  type GarbageQueue = USeq.USeq
  datatype PredecessorEdge = PredecessorEdge(src: BT.G.Reference, ghost idx: int)
  datatype IndirectionTable = IndirectionTable(t: HashMap, garbageQueue: lOption<GarbageQueue>, refUpperBound: uint64, findLoclessIterator: Option<LinearMutableMap.SimpleIterator>, ghost locs: map<BT.G.Reference, Location>, ghost graph: map<BT.G.Reference, seq<BT.G.Reference>>, ghost predCounts: map<BT.G.Reference, int>) {
    function I(): SectorType.IndirectionTable
      decreases this
    {
      SectorType.IndirectionTable(this.locs, this.graph)
    }
    predicate {:opaque} {:fuel 0, 0} Inv()
      ensures Inv() ==> this.locs.Keys <= this.graph.Keys
      decreases this
    {
      LinearMutableMap.Inv(this.t) &&
      this.locs == Locs(this.t) &&
      this.graph == Graph(this.t) &&
      this.predCounts == PredCounts(this.t) &&
      (this.garbageQueue.lSome? ==>
        this.garbageQueue.value.Inv()) &&
      ValidPredCounts(this.predCounts, this.graph) &&
      BC.GraphClosed(this.graph) &&
      (forall ref: NativeTypes.uint64 {:trigger this.graph[ref]} {:trigger ref in this.graph} | ref in this.graph :: 
        |this.graph[ref]| <= MaxNumChildren()) &&
      (this.garbageQueue.lSome? ==>
        (forall ref: uint64 {:trigger ref in this.garbageQueue.value.I()} {:trigger this.t.contents[ref]} {:trigger ref in this.t.contents} | ref in this.t.contents && this.t.contents[ref].predCount == 0 :: 
          ref in this.garbageQueue.value.I()) &&
        forall ref: uint64 {:trigger this.t.contents[ref]} {:trigger ref in this.t.contents} {:trigger ref in this.garbageQueue.value.I()} | ref in this.garbageQueue.value.I() :: 
          ref in this.t.contents &&
          this.t.contents[ref].predCount == 0) &&
      BT.G.Root() in this.t.contents &&
      this.t.count as int <= MaxSize() &&
      (forall ref: uint64 {:trigger ref in this.graph} | ref in this.graph :: 
        ref <= this.refUpperBound) &&
      (this.findLoclessIterator.Some? ==>
        LinearMutableMap.WFSimpleIter(this.t, this.findLoclessIterator.value) &&
        forall r: uint64 {:trigger r in this.locs} {:trigger r in this.findLoclessIterator.value.s} | r in this.findLoclessIterator.value.s :: 
          r in this.locs)
    }
    lemma  UpperBounded()
      requires Inv()
      ensures forall ref: uint64 {:trigger ref in this.graph} | ref in this.graph :: ref <= this.refUpperBound
      decreases this
    {
      reveal Inv();
    }
    static method Alloc(loc: Location) returns (r: IndirectionTable)
      ensures r.Inv()
      ensures r.graph == map[BT.G.Root() := []]
      ensures r.locs == map[BT.G.Root() := loc]
      ensures r.refUpperBound == BT.G.Root()
      decreases loc
    {
      var hashMap: LinearHashMap<Entry> := LinearMutableMap.Constructor<Entry>(128);
      var _inout_tmp_0: LinearHashMap<Entry>;
      _inout_tmp_0 := LinearMutableMap.Insert(inout hashMap, BT.G.Root(), Entry(Some(loc), [], 1));
      hashMap := _inout_tmp_0;
      r := IndirectionTable(hashMap, lNone, BT.G.Root(), None, Locs(hashMap), Graph(hashMap), PredCounts(hashMap));
      assert r.Inv() by {
        reveal r.Inv();
        reveal_PredCounts();
      }
    }
    static method AllocEmpty() returns (r: IndirectionTable)
      ensures r.Inv()
    {
      var hashMap: LinearHashMap<Entry> := LinearMutableMap.Constructor<Entry>(128);
      var _inout_tmp_0: LinearHashMap<Entry>;
      _inout_tmp_0 := LinearMutableMap.Insert(inout hashMap, BT.G.Root(), Entry(None, [], 1));
      hashMap := _inout_tmp_0;
      r := IndirectionTable(hashMap, lNone, 0, None, Locs(hashMap), Graph(hashMap), PredCounts(hashMap));
      assert r.Inv() by {
        reveal r.Inv();
        reveal_PredCounts();
      }
    }
    method Free()
      decreases this
    {
      var IndirectionTable(t: HashMap, garbageQueue: lOption<GarbageQueue>, refUpperBound: uint64, findLoclessIterator: Option<LinearMutableMap.SimpleIterator>, _: map<BT.G.Reference, Location>, _: map<BT.G.Reference, seq<BT.G.Reference>>, _: map<BT.G.Reference, int>) := this;
      LinearMutableMap.Destructor(t);
      match garbageQueue {
        case lNone() =>
          {
          }
        case lSome(gq) =>
          {
            gq.Free();
          }
      }
    }
    method Clone() returns (cloned: IndirectionTable)
      requires this.Inv()
      ensures cloned.Inv()
      ensures cloned.graph == this.graph
      ensures cloned.locs == this.locs
      ensures cloned.I() == this.I()
      decreases this
    {
      reveal Inv();
      var IndirectionTable(t: HashMap, garbageQueue: lOption<GarbageQueue>, refUpperBound: uint64, findLoclessIterator: Option<LinearMutableMap.SimpleIterator>, locs: map<BT.G.Reference, Location>, graph: map<BT.G.Reference, seq<BT.G.Reference>>, predCounts: map<BT.G.Reference, int>) := this;
      var t': LinearHashMap<Entry> := LinearMutableMap.Clone(t);
      cloned := IndirectionTable(t', lNone, refUpperBound, None, locs, graph, predCounts);
    }
    method GetEntry(ref: BT.G.Reference) returns (e: Option<Entry>)
      requires this.Inv()
      ensures e.None? ==> ref !in this.graph
      ensures e.Some? ==> ref in this.graph
      ensures e.Some? ==> this.graph[ref] == e.value.succs
      ensures e.Some? && e.value.loc.Some? ==> ref in this.locs && this.locs[ref] == e.value.loc.value
      ensures ref in this.locs ==> e.Some? && e.value.loc.Some?
      decreases this, ref
    {
      reveal Inv();
      e := LinearMutableMap.Get(this.t, ref);
    }
    method HasEmptyLoc(ref: BT.G.Reference) returns (b: bool)
      requires this.Inv()
      ensures b == (ref in this.graph && ref !in this.locs)
      decreases this, ref
    {
      reveal Inv();
      var entry: Option<Entry> := LinearMutableMap.Get(this.t, ref);
      b := entry.Some? && entry.value.loc.None?;
    }
    predicate TrackingGarbage()
      decreases this
    {
      this.garbageQueue.lSome?
    }
    method UpdateGhost(inout old_self: IndirectionTable) returns (self: IndirectionTable)
      ensures self == old_self.(locs := Locs(self.t), graph := Graph(self.t), predCounts := PredCounts(self.t))
      decreases this, old_self
    {
      self := old_self;
      ghost var _inout_tmp_0: map<BT.G.Reference, Location> := Locs(self.t);
      self := self.(locs := _inout_tmp_0);
      ghost var _inout_tmp_1: map<BT.G.Reference, seq<BT.G.Reference>> := Graph(self.t);
      self := self.(graph := _inout_tmp_1);
      ghost var _inout_tmp_2: map<BT.G.Reference, int> := PredCounts(self.t);
      self := self.(predCounts := _inout_tmp_2);
    }
    method RemoveLoc(inout old_self: IndirectionTable, ref: BT.G.Reference)
        returns (oldLoc: Option<Location>, self: IndirectionTable)
      requires old_self.Inv()
      requires old_self.TrackingGarbage()
      requires ref in old_self.graph
      ensures self.Inv()
      ensures self.locs == MapRemove1(old_self.locs, ref)
      ensures self.TrackingGarbage()
      ensures self.graph == old_self.graph
      ensures self.refUpperBound == old_self.refUpperBound
      ensures oldLoc.None? ==> ref !in old_self.locs
      ensures oldLoc.Some? ==> ref in old_self.locs && old_self.locs[ref] == oldLoc.value
      decreases this, old_self, ref
    {
      self := old_self;
      reveal old_self.Inv();
      var it: SimpleIterator := LinearMutableMap.FindSimpleIter(self.t, ref);
      var oldEntry: IteratorOutput<Entry> := LinearMutableMap.SimpleIterOutput(self.t, it);
      var predCount: uint64 := oldEntry.value.predCount;
      var succs: seq<BT.G.Reference> := oldEntry.value.succs;
      var _inout_tmp_0: LinearHashMap<Entry>;
      _inout_tmp_0 := LinearMutableMap.UpdateByIter(inout self.t, it, Entry(None, succs, predCount));
      self := self.(t := _inout_tmp_0);
      var _inout_tmp_1: Option<LinearMutableMap.SimpleIterator> := None;
      self := self.(findLoclessIterator := _inout_tmp_1);
      var _inout_tmp_2: IndirectionTable;
      _inout_tmp_2 := self.UpdateGhost(inout self);
      self := _inout_tmp_2;
      oldLoc := oldEntry.value.loc;
      assert PredCounts(self.t) == PredCounts(old_self.t) by {
        reveal_PredCounts();
      }
      assert Graph(self.t) == Graph(old_self.t);
    }
    method AddLocIfPresent(inout old_self: IndirectionTable, ref: BT.G.Reference, loc: Location)
        returns (added: bool, self: IndirectionTable)
      requires old_self.Inv()
      ensures self.Inv()
      ensures added == (ref in old_self.graph && ref !in old_self.locs)
      ensures self.graph == old_self.graph
      ensures self.refUpperBound == old_self.refUpperBound
      ensures added ==> self.locs == old_self.locs[ref := loc]
      ensures !added ==> self.locs == old_self.locs
      ensures old_self.TrackingGarbage() ==> self.TrackingGarbage()
      decreases this, old_self, ref, loc
    {
      self := old_self;
      reveal old_self.Inv();
      var it: SimpleIterator := LinearMutableMap.FindSimpleIter(self.t, ref);
      var oldEntry: IteratorOutput<Entry> := LinearMutableMap.SimpleIterOutput(self.t, it);
      added := oldEntry.Next? && oldEntry.value.loc.None?;
      if added {
        var _inout_tmp_0: LinearHashMap<Entry>;
        _inout_tmp_0 := LinearMutableMap.UpdateByIter(inout self.t, it, Entry(Some(loc), oldEntry.value.succs, oldEntry.value.predCount));
        self := self.(t := _inout_tmp_0);
        var _inout_tmp_1: IndirectionTable;
        _inout_tmp_1 := self.UpdateGhost(inout self);
        self := _inout_tmp_1;
      }
      assert Graph(self.t) == Graph(old_self.t);
      assert self.Inv() by {
        reveal self.Inv();
        reveal_PredCounts();
      }
    }
    predicate DeallocableRef(ref: BT.G.Reference)
      decreases this, ref
    {
      ref in this.graph &&
      ref != BT.G.Root() &&
      forall r: NativeTypes.uint64 {:trigger this.graph[r]} {:trigger r in this.graph} | r in this.graph :: 
        ref !in this.graph[r]
    }
    static lemma TCountEqGraphSize(t: HashMap)
      requires LinearMutableMap.Inv(t)
      ensures t.count as int == |Graph(t)|
      decreases t
    {
      assert Graph(t).Keys == t.contents.Keys;
      assert |Graph(t)| == |Graph(t).Keys| == |t.contents.Keys| == t.count as int;
    }
    static function SeqCountSet(s: seq<BT.G.Reference>, ref: BT.G.Reference, lb: int): set<int>
      requires 0 <= lb <= |s|
      decreases s, ref, lb
    {
      set i: int {:trigger s[i]} | lb <= i < |s| && s[i] == ref
    }
    static function SeqCount(s: seq<BT.G.Reference>, ref: BT.G.Reference, lb: int): int
      requires 0 <= lb <= |s|
      decreases s, ref, lb
    {
      |SeqCountSet(s, ref, lb)|
    }
    static function PredecessorSetExcept(graph: map<BT.G.Reference, seq<BT.G.Reference>>, dest: BT.G.Reference, except: BT.G.Reference): set<PredecessorEdge>
      decreases graph, dest, except
    {
      set src: NativeTypes.uint64, idx: int {:trigger PredecessorEdge(src, idx)} {:trigger graph[src][idx]} | src in graph && 0 <= idx < |graph[src]| && graph[src][idx] == dest && src != except :: PredecessorEdge(src, idx)
    }
    static lemma SeqCountPlusPredecessorSetExcept(graph: map<BT.G.Reference, seq<BT.G.Reference>>, dest: BT.G.Reference, except: BT.G.Reference)
      ensures ghost var succs: seq<NativeTypes.uint64> := if except in graph then graph[except] else []; SeqCount(succs, dest, 0) + |PredecessorSetExcept(graph, dest, except)| == |PredecessorSet(graph, dest)|
      decreases graph, dest, except
    {
      ghost var succs: seq<NativeTypes.uint64> := if except in graph then graph[except] else [];
      ghost var a1: set<int> := SeqCountSet(succs, dest, 0);
      ghost var a: set<PredecessorEdge> := set src: NativeTypes.uint64, idx: int {:trigger PredecessorEdge(src, idx)} {:trigger graph[src][idx]} | src in graph && 0 <= idx < |graph[src]| && graph[src][idx] == dest && src == except :: PredecessorEdge(src, idx);
      ghost var b: set<PredecessorEdge> := PredecessorSetExcept(graph, dest, except);
      ghost var c: set<PredecessorEdge> := PredecessorSet(graph, dest);
      assert a + b == c;
      assert a !! b;
      assert |a| + |b| == |c|;
      ghost var relation: iset<(PredecessorEdge, int)> := iset p: (PredecessorEdge, int) {:trigger p.1} {:trigger p.0} | p.0.idx == p.1;
      forall x: PredecessorEdge {:trigger x in a} | x in a
        ensures exists y: int {:trigger (x, y)} {:trigger y in a1} :: y in a1 && (x, y) in relation
      {
        ghost var y: int := x.idx;
        assert y in a1 && (x, y) in relation;
      }
      forall y: int {:trigger y in a1} | y in a1
        ensures exists x: PredecessorEdge {:trigger (x, y)} {:trigger x in a} :: x in a && (x, y) in relation
      {
        ghost var x: PredecessorEdge := PredecessorEdge(except, y);
        assert x in a && (x, y) in relation;
      }
      SetBijectivity.BijectivityImpliesEqualCardinality(a, a1, relation);
      assert |a| == |a1|;
    }
    static predicate ValidPredCountsIntermediate(predCounts: map<BT.G.Reference, int>, graph: map<BT.G.Reference, seq<BT.G.Reference>>, newSuccs: seq<BT.G.Reference>, oldSuccs: seq<BT.G.Reference>, newIdx: int, oldIdx: int)
      requires 0 <= newIdx <= |newSuccs|
      requires 0 <= oldIdx <= |oldSuccs|
      decreases predCounts, graph, newSuccs, oldSuccs, newIdx, oldIdx
    {
      forall ref: NativeTypes.uint64 {:trigger SeqCount(oldSuccs, ref, oldIdx)} {:trigger SeqCount(newSuccs, ref, newIdx)} {:trigger IsRoot(ref)} {:trigger PredecessorSet(graph, ref)} {:trigger predCounts[ref]} {:trigger ref in predCounts} | ref in predCounts :: 
        predCounts[ref] == |PredecessorSet(graph, ref)| + IsRoot(ref) - SeqCount(newSuccs, ref, newIdx) + SeqCount(oldSuccs, ref, oldIdx)
    }
    static predicate RefcountUpdateInv(t: HashMap, garbageQueueI: seq<uint64>, changingRef: BT.G.Reference, newSuccs: seq<BT.G.Reference>, oldSuccs: seq<BT.G.Reference>, newIdx: int, oldIdx: int)
      decreases t, garbageQueueI, changingRef, newSuccs, oldSuccs, newIdx, oldIdx
    {
      LinearMutableMap.Inv(t) &&
      t.count as int <= MaxSize() &&
      |oldSuccs| <= MaxNumChildren() &&
      |newSuccs| <= MaxNumChildren() &&
      (forall ref: NativeTypes.uint64 {:trigger Graph(t)[ref]} {:trigger ref in Graph(t)} | ref in Graph(t) :: 
        |Graph(t)[ref]| <= MaxNumChildren()) &&
      0 <= newIdx <= |newSuccs| &&
      0 <= oldIdx <= |oldSuccs| &&
      (changingRef in Graph(t) ==>
        Graph(t)[changingRef] == newSuccs) &&
      (changingRef !in Graph(t) ==>
        newSuccs == []) &&
      ValidPredCountsIntermediate(PredCounts(t), Graph(t), newSuccs, oldSuccs, newIdx, oldIdx) &&
      (forall j: int {:trigger oldSuccs[j]} | 0 <= j < |oldSuccs| :: 
        oldSuccs[j] in t.contents) &&
      BC.GraphClosed(Graph(t)) &&
      (forall ref: uint64 {:trigger ref in Set(garbageQueueI)} {:trigger t.contents[ref]} {:trigger ref in t.contents} | ref in t.contents && t.contents[ref].predCount == 0 :: 
        ref in Set(garbageQueueI)) &&
      (forall ref: uint64 {:trigger t.contents[ref]} {:trigger ref in t.contents} {:trigger ref in Set(garbageQueueI)} | ref in Set(garbageQueueI) :: 
        ref in t.contents &&
        t.contents[ref].predCount == 0) &&
      BT.G.Root() in t.contents
    }
    method RemoveRef(inout old_self: IndirectionTable, ref: BT.G.Reference)
        returns (oldLoc: Option<Location>, self: IndirectionTable)
      requires old_self.Inv()
      requires old_self.TrackingGarbage()
      requires old_self.DeallocableRef(ref)
      ensures self.Inv()
      ensures self.TrackingGarbage()
      ensures self.graph == MapRemove1(old_self.graph, ref)
      ensures self.locs == MapRemove1(old_self.locs, ref)
      ensures ref in old_self.locs ==> oldLoc == Some(old_self.locs[ref])
      ensures ref !in old_self.locs ==> oldLoc == None
      ensures self.refUpperBound == old_self.refUpperBound
      decreases this, old_self, ref
    {
      self := old_self;
      reveal old_self.Inv();
      TCountEqGraphSize(self.t);
      var oldEntry: Option<Entry>, _inout_tmp_0: LinearHashMap<Entry> := LinearMutableMap.RemoveAndGet(inout self.t, ref);
      self := self.(t := _inout_tmp_0);
      TCountEqGraphSize(self.t);
      assert |Graph(old_self.t)[ref]| <= MaxNumChildren();
      forall ref: NativeTypes.uint64 {:trigger Graph(self.t)[ref]} {:trigger ref in Graph(self.t)} | ref in Graph(self.t)
        ensures |Graph(self.t)[ref]| <= MaxNumChildren()
      {
        assert Graph(self.t)[ref] == Graph(old_self.t)[ref];
      }
      ghost var graph0: map<BT.G.Reference, seq<BT.G.Reference>> := Graph(old_self.t);
      ghost var graph1: map<BT.G.Reference, seq<BT.G.Reference>> := Graph(self.t);
      ghost var succs0: seq<BT.G.Reference> := Graph(old_self.t)[ref];
      ghost var succs1: seq<NativeTypes.uint64> := [];
      ghost var predCounts1: map<BT.G.Reference, int> := PredCounts(self.t);
      forall r: NativeTypes.uint64 {:trigger SeqCount(succs0, r, 0)} {:trigger SeqCount(succs1, r, 0)} {:trigger IsRoot(r)} {:trigger PredecessorSet(graph1, r)} {:trigger predCounts1[r]} {:trigger r in predCounts1} | r in predCounts1
        ensures predCounts1[r] == |PredecessorSet(graph1, r)| + IsRoot(r) - SeqCount(succs1, r, 0) + SeqCount(succs0, r, 0)
      {
        reveal_PredCounts();
        SeqCountPlusPredecessorSetExcept(graph0, r, ref);
        SeqCountPlusPredecessorSetExcept(graph1, r, ref);
        assert PredecessorSetExcept(graph0, r, ref) == PredecessorSetExcept(graph1, r, ref);
      }
      assert ValidPredCountsIntermediate(PredCounts(self.t), Graph(self.t), [], succs0, 0, 0);
      forall j: int {:trigger succs0[j]} | 0 <= j < |succs0|
        ensures succs0[j] in self.t.contents
      {
        if succs0[j] == ref {
          assert ref in old_self.graph[ref];
          assert false;
        }
        assert succs0[j] == old_self.t.contents[ref].succs[j];
        assert succs0[j] in old_self.t.contents[ref].succs;
        assert succs0[j] in old_self.t.contents;
      }
      forall r: NativeTypes.uint64, succ: NativeTypes.uint64 {:trigger succ in Graph(self.t), Graph(self.t)[r]} {:trigger succ in Graph(self.t), r in Graph(self.t)} | r in Graph(self.t) && succ in Graph(self.t)[r]
        ensures succ in Graph(self.t)
      {
        if succ == ref {
          assert ref in old_self.graph[r];
          assert false;
        }
        assert succ in Graph(old_self.t)[r];
        assert succ in Graph(old_self.t);
      }
      var _inout_tmp_1: USeq;
      _inout_tmp_1 := self.garbageQueue.value.Remove(inout self.garbageQueue.value, ref);
      self := self.(garbageQueue := self.garbageQueue.(value := _inout_tmp_1));
      assert self.t.count as int <= MaxSize();
      var _inout_tmp_2: IndirectionTable;
      _inout_tmp_2 := self.UpdatePredCounts(inout self, ref, [], oldEntry.value.succs);
      self := _inout_tmp_2;
      TCountEqGraphSize(self.t);
      oldLoc := if oldEntry.Some? then oldEntry.value.loc else None;
      var _inout_tmp_3: Option<LinearMutableMap.SimpleIterator> := None;
      self := self.(findLoclessIterator := _inout_tmp_3);
      var _inout_tmp_4: IndirectionTable;
      _inout_tmp_4 := self.UpdateGhost(inout self);
      self := _inout_tmp_4;
      assert self.graph == MapRemove1(old_self.graph, ref);
      reveal self.Inv();
    }
    static predicate UnchangedExceptTAndGarbageQueue(old_self: IndirectionTable, self: IndirectionTable)
      decreases old_self, self
    {
      self == old_self.(t := self.t, garbageQueue := self.garbageQueue)
    }
    method PredInc(inout old_self: IndirectionTable, ref: BT.G.Reference) returns (self: IndirectionTable)
      requires old_self.t.Inv()
      requires old_self.TrackingGarbage()
      requires old_self.garbageQueue.value.Inv()
      requires old_self.t.count as nat < 18446744073709551616 / 8
      requires ref in old_self.t.contents
      requires old_self.t.contents[ref].predCount < 18446744073709551615
      ensures self.t.Inv()
      ensures self.TrackingGarbage()
      ensures self.garbageQueue.value.Inv()
      ensures UnchangedExceptTAndGarbageQueue(old_self, self)
      ensures var oldEntry: Entry := old_self.t.contents[ref]; self.t.contents == old_self.t.contents[ref := oldEntry.(predCount := oldEntry.predCount + 1)]
      ensures if old_self.t.contents[ref].predCount == 0 then self.garbageQueue.value.I() == RemoveOneValue(old_self.garbageQueue.value.I(), ref) else self.garbageQueue.value.I() == old_self.garbageQueue.value.I()
      decreases this, old_self, ref
    {
      self := old_self;
      var oldEntryOpt: Option<Entry> := LinearMutableMap.Get(self.t, ref);
      var oldEntry: Entry := oldEntryOpt.value;
      var newEntry: Entry := oldEntry.(predCount := oldEntry.predCount + 1);
      var _inout_tmp_1: LinearHashMap<Entry>;
      _inout_tmp_1 := LinearMutableMap.Insert(inout self.t, ref, newEntry);
      self := self.(t := _inout_tmp_1);
      if oldEntry.predCount == 0 {
        var _inout_tmp_0: USeq;
        _inout_tmp_0 := self.garbageQueue.value.Remove(inout self.garbageQueue.value, ref);
        self := self.(garbageQueue := self.garbageQueue.(value := _inout_tmp_0));
      }
    }
    method PredDec(inout old_self: IndirectionTable, ref: BT.G.Reference) returns (self: IndirectionTable)
      requires old_self.t.Inv()
      requires old_self.TrackingGarbage()
      requires old_self.garbageQueue.value.Inv()
      requires old_self.t.count as nat < 18446744073709551616 / 8
      requires ref in old_self.t.contents
      requires old_self.t.contents[ref].predCount > 0
      requires |old_self.garbageQueue.value.I()| <= 4294967296
      ensures self.t.Inv()
      ensures self.TrackingGarbage()
      ensures self.garbageQueue.value.Inv()
      ensures UnchangedExceptTAndGarbageQueue(old_self, self)
      ensures var oldEntry: Entry := old_self.t.contents[ref]; self.t.contents == old_self.t.contents[ref := oldEntry.(predCount := oldEntry.predCount - 1)]
      ensures if old_self.t.contents[ref].predCount == 1 && ref !in old_self.garbageQueue.value.I() then self.garbageQueue.value.I() == old_self.garbageQueue.value.I() + [ref] else self.garbageQueue.value.I() == old_self.garbageQueue.value.I()
      decreases this, old_self, ref
    {
      self := old_self;
      var oldEntryOpt: Option<Entry> := LinearMutableMap.Get(self.t, ref);
      var oldEntry: Entry := oldEntryOpt.value;
      var newEntry: Entry := oldEntry.(predCount := oldEntry.predCount - 1);
      var _inout_tmp_1: LinearHashMap<Entry>;
      _inout_tmp_1 := LinearMutableMap.Insert(inout self.t, ref, newEntry);
      self := self.(t := _inout_tmp_1);
      if oldEntry.predCount == 1 {
        var _inout_tmp_0: USeq;
        _inout_tmp_0 := self.garbageQueue.value.Add(inout self.garbageQueue.value, ref);
        self := self.(garbageQueue := self.garbageQueue.(value := _inout_tmp_0));
      }
    }
    static lemma SeqCountLePredecessorSet(graph: map<BT.G.Reference, seq<BT.G.Reference>>, ref: BT.G.Reference, r: BT.G.Reference, lb: int)
      requires r in graph
      requires 0 <= lb <= |graph[r]|
      ensures SeqCount(graph[r], ref, lb) <= |PredecessorSet(graph, ref)|
      decreases graph, ref, r, lb
    {
      ghost var setA: set<int> := set i: int {:trigger graph[r][i]} | lb <= i < |graph[r]| && graph[r][i] == ref;
      ghost var setB: set<PredecessorEdge> := set src: NativeTypes.uint64, idx: int {:trigger PredecessorEdge(src, idx)} {:trigger graph[src][idx]} | src in graph && 0 <= idx < |graph[src]| && graph[src][idx] == ref && src == r && lb <= idx :: PredecessorEdge(src, idx);
      ghost var setC: set<PredecessorEdge> := set src: NativeTypes.uint64, idx: int {:trigger PredecessorEdge(src, idx)} {:trigger graph[src][idx]} | src in graph && 0 <= idx < |graph[src]| && graph[src][idx] == ref :: PredecessorEdge(src, idx);
      calc == {
        |SeqCountSet(graph[r], ref, lb)|;
      ==
        |setA|;
      ==
        {
          ghost var relation: iset<(int, PredecessorEdge)> := iset i: int, src: NativeTypes.uint64, idx: int {:trigger (i, PredecessorEdge(src, idx))} | src == r && i == idx :: (i, PredecessorEdge(src, idx));
          forall a: int {:trigger a in setA} | a in setA
            ensures exists b: PredecessorEdge {:trigger (a, b)} {:trigger b in setB} :: b in setB && (a, b) in relation
          {
            ghost var b: PredecessorEdge := PredecessorEdge(r, a);
            assert b in setB;
            assert (a, b) in relation;
          }
          forall b: PredecessorEdge {:trigger b in setB} | b in setB
            ensures exists a: int {:trigger (a, b)} {:trigger a in setA} :: a in setA && (a, b) in relation
          {
            ghost var a: int := b.idx;
            assert a in setA;
            assert (a, b) in relation;
          }
          SetBijectivity.BijectivityImpliesEqualCardinality(setA, setB, relation);
        }
        |setB|;
      }
      SetInclusionImpliesSmallerCardinality(setB, setC);
    }
    static lemma SeqCountInc(s: seq<BT.G.Reference>, ref: BT.G.Reference, idx: int)
      requires 0 <= idx < |s|
      requires s[idx] == ref
      ensures SeqCount(s, ref, idx + 1) == SeqCount(s, ref, idx) - 1
      decreases s, ref, idx
    {
      ghost var a: set<int> := set i: int {:trigger s[i]} | idx <= i < |s| && s[i] == ref;
      ghost var b: set<int> := set i: int {:trigger s[i]} | idx + 1 <= i < |s| && s[i] == ref;
      assert a == b + {idx};
    }
    static lemma SeqCountIncOther(s: seq<BT.G.Reference>, ref: BT.G.Reference, idx: int)
      requires 0 <= idx < |s|
      requires s[idx] != ref
      ensures SeqCount(s, ref, idx + 1) == SeqCount(s, ref, idx)
      decreases s, ref, idx
    {
      ghost var a: set<int> := set i: int {:trigger s[i]} | idx <= i < |s| && s[i] == ref;
      ghost var b: set<int> := set i: int {:trigger s[i]} | idx + 1 <= i < |s| && s[i] == ref;
      assert a == b;
    }
    static lemma PredecessorSetRestrictedSizeBound(graph: map<BT.G.Reference, seq<BT.G.Reference>>, dest: BT.G.Reference, domain: set<BT.G.Reference>)
      requires |graph| <= MaxSize()
      requires forall ref: NativeTypes.uint64 {:trigger graph[ref]} {:trigger ref in graph} | ref in graph :: |graph[ref]| <= MaxNumChildren()
      ensures |PredecessorSetRestricted(graph, dest, domain)| <= MaxSize() * MaxNumChildren()
      decreases graph, dest, domain
    {
      ghost var s1: set<PredecessorEdge> := set src: NativeTypes.uint64, idx: int {:trigger PredecessorEdge(src, idx)} {:trigger graph[src][idx]} | src in graph && 0 <= idx < |graph[src]| && graph[src][idx] == dest && src in domain :: PredecessorEdge(src, idx);
      ghost var s2: set<PredecessorEdge> := set src: NativeTypes.uint64, idx: int {:trigger PredecessorEdge(src, idx)} {:trigger idx in SetRange(MaxNumChildren()), src in graph.Keys} | src in graph.Keys && idx in SetRange(MaxNumChildren()) :: PredecessorEdge(src, idx);
      ghost var s3: set<(NativeTypes.uint64, int)> := set src: NativeTypes.uint64, idx: int {:trigger (src, idx)} {:trigger idx in SetRange(MaxNumChildren()), src in graph.Keys} | src in graph.Keys && idx in SetRange(MaxNumChildren()) :: (src, idx);
      assert s1 <= s2;
      SetInclusionImpliesSmallerCardinality(s1, s2);
      assert |s1| <= |s2|;
      ghost var relation: iset<(PredecessorEdge, (BT.G.Reference, int))> := iset t: (PredecessorEdge, (BT.G.Reference, int)) {:trigger t.1} {:trigger t.0} | t.0.src == t.1.0 && t.0.idx == t.1.1;
      forall a: PredecessorEdge {:trigger a in s2} | a in s2
        ensures exists b: (NativeTypes.uint64, int) {:trigger (a, b)} {:trigger b in s3} :: b in s3 && (a, b) in relation
      {
        ghost var b: (BT.G.Reference, int) := (a.src, a.idx);
        assert b in s3;
      }
      forall b: (NativeTypes.uint64, int) {:trigger b in s3} | b in s3
        ensures exists a: PredecessorEdge {:trigger (a, b)} {:trigger a in s2} :: a in s2 && (a, b) in relation
      {
        ghost var a: PredecessorEdge := PredecessorEdge(b.0, b.1);
        assert a in s2;
      }
      SetBijectivity.BijectivityImpliesEqualCardinality(s2, s3, relation);
      assert |s2| == |s3|;
      ghost var x1: set<BT.G.Reference> := graph.Keys;
      ghost var y1: set<int> := SetRange(MaxNumChildren());
      ghost var z1: set<(NativeTypes.uint64, int)> := set a: NativeTypes.uint64, b: int {:trigger (a, b)} {:trigger b in y1, a in x1} | a in x1 && b in y1 :: (a, b);
      SetBijectivity.CrossProductCardinality(x1, y1, z1);
      assert |s3| == |z1| == |x1| * |y1| == |graph.Keys| * |SetRange(MaxNumChildren())|;
      assert |graph.Keys| <= MaxSize();
      CardinalitySetRange(MaxNumChildren());
      assert |SetRange(MaxNumChildren())| == MaxNumChildren();
      assert |PredecessorSetRestricted(graph, dest, domain)| == |s1| <= |s3| == |graph.Keys| * MaxNumChildren();
    }
    static lemma PredecessorSetSizeBound(graph: map<BT.G.Reference, seq<BT.G.Reference>>, dest: BT.G.Reference)
      requires |graph| <= MaxSize()
      requires forall ref: NativeTypes.uint64 {:trigger graph[ref]} {:trigger ref in graph} | ref in graph :: |graph[ref]| <= MaxNumChildren()
      ensures |PredecessorSet(graph, dest)| <= MaxSize() * MaxNumChildren()
      decreases graph, dest
    {
      PredecessorSetRestrictedSizeBound(graph, dest, graph.Keys);
      assert PredecessorSet(graph, dest) == PredecessorSetRestricted(graph, dest, graph.Keys);
    }
    static lemma SeqCountBound(s: seq<BT.G.Reference>, ref: BT.G.Reference, lb: int)
      requires 0 <= lb <= |s|
      ensures SeqCount(s, ref, lb) <= |s|
      decreases s, ref, lb
    {
      ghost var s1: set<int> := SeqCountSet(s, ref, lb);
      ghost var s2: set<int> := SetRange(|s|);
      assert s1 <= s2;
      SetInclusionImpliesSmallerCardinality(s1, s2);
      CardinalitySetRange(|s|);
    }
    method UpdatePredCounts(inout old_self: IndirectionTable, ghost changingRef: BT.G.Reference, newSuccs: seq<BT.G.Reference>, oldSuccs: seq<BT.G.Reference>)
        returns (self: IndirectionTable)
      requires old_self.t.Inv()
      requires old_self.TrackingGarbage()
      requires old_self.garbageQueue.value.Inv()
      requires RefcountUpdateInv(old_self.t, old_self.garbageQueue.value.I(), changingRef, newSuccs, oldSuccs, 0, 0)
      ensures self.t.Inv()
      ensures self.TrackingGarbage()
      ensures self.garbageQueue.value.Inv()
      ensures RefcountUpdateInv(self.t, self.garbageQueue.value.I(), changingRef, newSuccs, oldSuccs, |newSuccs|, |oldSuccs|)
      ensures UnchangedExceptTAndGarbageQueue(old_self, self)
      ensures Graph(self.t) == Graph(old_self.t)
      ensures Locs(self.t) == Locs(old_self.t)
      decreases this, old_self, changingRef, newSuccs, oldSuccs
    {
      self := old_self;
      var idx: uint64 := 0;
      while idx < |newSuccs| as uint64
        invariant self.t.Inv()
        invariant self.TrackingGarbage()
        invariant self.garbageQueue.value.Inv()
        invariant RefcountUpdateInv(self.t, self.garbageQueue.value.I(), changingRef, newSuccs, oldSuccs, idx as int, 0)
        invariant UnchangedExceptTAndGarbageQueue(old_self, self)
        invariant Graph(self.t) == Graph(old_self.t)
        invariant Locs(self.t) == Locs(old_self.t)
        decreases |newSuccs| - idx as int
      {
        var ref: NativeTypes.uint64 := newSuccs[idx];
        ghost var graph: map<BT.G.Reference, seq<BT.G.Reference>> := Graph(self.t);
        assert ref in graph;
        assert ref in self.t.contents;
        assert self.t.contents[ref].predCount as int == |PredecessorSet(graph, ref)| + IsRoot(ref) - SeqCount(newSuccs, ref, idx as nat) + SeqCount(oldSuccs, ref, 0) by {
          reveal_PredCounts();
        }
        SeqCountInc(newSuccs, ref, idx as nat);
        assert SeqCount(newSuccs, ref, idx as nat + 1) == SeqCount(newSuccs, ref, idx as nat) - 1;
        TCountEqGraphSize(self.t);
        PredecessorSetSizeBound(graph, ref);
        SeqCountBound(oldSuccs, ref, 0);
        assert self.t.contents[ref].predCount < 18446744073709551615;
        ghost var self_before: IndirectionTable := self;
        var _inout_tmp_0: IndirectionTable;
        _inout_tmp_0 := self.PredInc(inout self, ref);
        self := _inout_tmp_0;
        assert Graph(self.t) == Graph(self_before.t);
        ghost var predCounts: map<BT.G.Reference, int> := PredCounts(self_before.t);
        ghost var predCounts': map<BT.G.Reference, int> := PredCounts(self.t);
        forall r: NativeTypes.uint64 {:trigger SeqCount(oldSuccs, r, 0)} {:trigger IsRoot(r)} {:trigger PredecessorSet(graph, r)} {:trigger predCounts'[r]} {:trigger r in predCounts'} | r in predCounts'
          ensures predCounts'[r] == |PredecessorSet(graph, r)| + IsRoot(r) - SeqCount(newSuccs, r, idx as nat + 1) + SeqCount(oldSuccs, r, 0)
        {
          reveal_PredCounts();
          if r == ref {
          } else {
            SeqCountIncOther(newSuccs, r, idx as nat);
          }
        }
        assert RefcountUpdateInv(self.t, self.garbageQueue.value.I(), changingRef, newSuccs, oldSuccs, idx as nat + 1, 0);
        idx := idx + 1;
      }
      var idx2: uint64 := 0;
      while idx2 < |oldSuccs| as uint64
        invariant self.t.Inv()
        invariant self.TrackingGarbage()
        invariant self.garbageQueue.value.Inv()
        invariant RefcountUpdateInv(self.t, self.garbageQueue.value.I(), changingRef, newSuccs, oldSuccs, |newSuccs|, idx2 as int)
        invariant UnchangedExceptTAndGarbageQueue(old_self, self)
        invariant Graph(self.t) == Graph(old_self.t)
        invariant Locs(self.t) == Locs(old_self.t)
        decreases |oldSuccs| - idx2 as int
      {
        assert Set(self.garbageQueue.value.I()) <= self.t.contents.Keys;
        SetInclusionImpliesSmallerCardinality(Set(self.garbageQueue.value.I()), self.t.contents.Keys);
        NoDupesSetCardinality(self.garbageQueue.value.I());
        assert |self.t.contents.Keys| == |self.t.contents|;
        ghost var graph: map<BT.G.Reference, seq<BT.G.Reference>> := Graph(self.t);
        assert oldSuccs[idx2] in graph;
        assert oldSuccs[idx2] in self.t.contents;
        ghost var ref: NativeTypes.uint64 := oldSuccs[idx2];
        assert self.t.contents[ref].predCount as int == |PredecessorSet(graph, ref)| + IsRoot(ref) - SeqCount(newSuccs, ref, |newSuccs|) + SeqCount(oldSuccs, ref, idx2 as nat) by {
          reveal_PredCounts();
        }
        if changingRef in graph {
          SeqCountLePredecessorSet(graph, ref, changingRef, |newSuccs|);
          assert |PredecessorSet(graph, ref)| >= SeqCount(graph[changingRef], ref, |newSuccs|);
        }
        SeqCountInc(oldSuccs, ref, idx2 as nat);
        assert SeqCount(oldSuccs, ref, idx2 as nat + 1) == SeqCount(oldSuccs, ref, idx2 as nat) - 1;
        assert self.t.contents[oldSuccs[idx2]].predCount > 0;
        ghost var self_before: IndirectionTable := self;
        var _inout_tmp_1: IndirectionTable;
        _inout_tmp_1 := self.PredDec(inout self, oldSuccs[idx2]);
        self := _inout_tmp_1;
        if self_before.t.contents[ref].predCount == 1 {
          assert NoDupes([ref]) by {
            reveal_NoDupes();
          }
          DisjointConcatenation(self_before.garbageQueue.value.I(), [ref]);
        }
        assert Graph(self_before.t) == Graph(self.t);
        ghost var predCounts: map<BT.G.Reference, int> := PredCounts(self_before.t);
        ghost var predCounts': map<BT.G.Reference, int> := PredCounts(self.t);
        forall r: NativeTypes.uint64 {:trigger IsRoot(r)} {:trigger PredecessorSet(graph, r)} {:trigger predCounts'[r]} {:trigger r in predCounts'} | r in predCounts'
          ensures predCounts'[r] == |PredecessorSet(graph, r)| + IsRoot(r) - SeqCount(newSuccs, r, |newSuccs| as nat) + SeqCount(oldSuccs, r, idx2 as nat + 1)
        {
          reveal_PredCounts();
          if r == ref {
          } else {
            SeqCountIncOther(oldSuccs, r, idx2 as nat);
            assert SeqCount(oldSuccs, r, idx2 as nat) == SeqCount(oldSuccs, r, idx2 as nat + 1);
          }
        }
        idx2 := idx2 + 1;
      }
    }
    static predicate SuccsValid(succs: seq<BT.G.Reference>, graph: map<BT.G.Reference, seq<BT.G.Reference>>)
      decreases succs, graph
    {
      forall ref: NativeTypes.uint64 {:trigger ref in graph} {:trigger ref in succs} | ref in succs :: 
        ref in graph
    }
    lemma  QueueSizeBound()
      requires this.Inv()
      ensures this.garbageQueue.lSome? ==> |this.garbageQueue.value.I()| <= 4294967296
      decreases this
    {
      reveal this.Inv();
      if this.garbageQueue.lSome? {
        NoDupesSetCardinality(this.garbageQueue.value.I());
        SetInclusionImpliesSmallerCardinality(Set(this.garbageQueue.value.I()), this.t.contents.Keys);
        assert |this.t.contents.Keys| == |this.t.contents|;
      }
    }
    method {:timeLimitMultiplier 2}  UpdateAndRemoveLoc(inout old_self: IndirectionTable, ref: BT.G.Reference, succs: seq<BT.G.Reference>)
        returns (oldLoc: Option<Location>, self: IndirectionTable)
      requires old_self.Inv()
      requires old_self.TrackingGarbage()
      requires |succs| <= MaxNumChildren()
      requires |old_self.graph| < MaxSize()
      requires SuccsValid(succs, old_self.graph)
      ensures self.Inv()
      ensures self.TrackingGarbage()
      ensures self.Inv()
      ensures self.TrackingGarbage()
      ensures self.locs == MapRemove1(old_self.locs, ref)
      ensures self.graph == old_self.graph[ref := succs]
      ensures self.refUpperBound == if ref > old_self.refUpperBound then ref else old_self.refUpperBound
      ensures oldLoc.None? ==> ref !in old_self.locs
      ensures oldLoc.Some? ==> ref in old_self.locs && old_self.locs[ref] == oldLoc.value
      decreases this, old_self, ref, succs
    {
      self := old_self;
      reveal old_self.Inv();
      self.QueueSizeBound();
      TCountEqGraphSize(self.t);
      var oldEntry: Option<Entry> := LinearMutableMap.Get(self.t, ref);
      var predCount: uint64 := if oldEntry.Some? then oldEntry.value.predCount else 0;
      if oldEntry.None? {
        var _inout_tmp_0: USeq;
        _inout_tmp_0 := self.garbageQueue.value.Add(inout self.garbageQueue.value, ref);
        self := self.(garbageQueue := self.garbageQueue.(value := _inout_tmp_0));
      }
      var _inout_tmp_2: LinearHashMap<Entry>;
      _inout_tmp_2 := LinearMutableMap.Insert(inout self.t, ref, Entry(None, succs, predCount));
      self := self.(t := _inout_tmp_2);
      assert oldEntry.Some? ==> oldEntry.value.succs == Graph(old_self.t)[ref];
      assert forall r: NativeTypes.uint64 {:trigger Graph(old_self.t)[r]} {:trigger Graph(self.t)[r]} {:trigger r in Graph(old_self.t)} {:trigger r in Graph(self.t)} | r != ref && r in Graph(self.t) :: r in Graph(old_self.t) && Graph(self.t)[r] == Graph(old_self.t)[r];
      TCountEqGraphSize(self.t);
      ghost var oldSuccs: seq<BT.G.Reference> := if oldEntry.Some? then oldEntry.value.succs else [];
      ghost var predCounts: map<BT.G.Reference, int> := PredCounts(self.t);
      ghost var graph0: map<BT.G.Reference, seq<BT.G.Reference>> := Graph(old_self.t);
      ghost var graph: map<BT.G.Reference, seq<BT.G.Reference>> := Graph(self.t);
      forall r: NativeTypes.uint64 {:trigger SeqCount(oldSuccs, r, 0)} {:trigger SeqCount(succs, r, 0)} {:trigger IsRoot(r)} {:trigger PredecessorSet(graph, r)} {:trigger predCounts[r]} {:trigger r in predCounts} | r in predCounts
        ensures predCounts[r] == |PredecessorSet(graph, r)| + IsRoot(r) - SeqCount(succs, r, 0) + SeqCount(oldSuccs, r, 0)
      {
        reveal_PredCounts();
        SeqCountPlusPredecessorSetExcept(graph0, r, ref);
        SeqCountPlusPredecessorSetExcept(graph, r, ref);
        assert PredecessorSetExcept(graph0, r, ref) == PredecessorSetExcept(graph, r, ref);
        assert |PredecessorSet(graph0, r)| - SeqCount(oldSuccs, r, 0) == |PredecessorSetExcept(graph0, r, ref)| == |PredecessorSetExcept(graph, r, ref)| == |PredecessorSet(graph, r)| - SeqCount(succs, r, 0);
      }
      assert ValidPredCountsIntermediate(PredCounts(self.t), Graph(self.t), succs, oldSuccs, 0, 0);
      forall j: int {:trigger oldSuccs[j]} | 0 <= j < |oldSuccs|
        ensures oldSuccs[j] in self.t.contents
      {
        assert oldSuccs[j] in graph0;
        assert oldSuccs[j] in graph;
      }
      assert RefcountUpdateInv(self.t, self.garbageQueue.value.I(), ref, succs, oldSuccs, 0, 0);
      var _inout_tmp_3: IndirectionTable;
      _inout_tmp_3 := self.UpdatePredCounts(inout self, ref, succs, if oldEntry.Some? then oldEntry.value.succs else []);
      self := _inout_tmp_3;
      TCountEqGraphSize(self.t);
      assert ValidPredCounts(PredCounts(self.t), Graph(self.t));
      if ref > self.refUpperBound {
        var _inout_tmp_1: BT.G.Reference := ref;
        self := self.(refUpperBound := _inout_tmp_1);
      }
      oldLoc := if oldEntry.Some? && oldEntry.value.loc.Some? then oldEntry.value.loc else None;
      var _inout_tmp_4: Option<LinearMutableMap.SimpleIterator> := None;
      self := self.(findLoclessIterator := _inout_tmp_4);
      var _inout_tmp_5: IndirectionTable;
      _inout_tmp_5 := self.UpdateGhost(inout self);
      self := _inout_tmp_5;
      reveal self.Inv();
    }
    static predicate ValIsHashMap(a: seq<V>, s: Option<HashMap>)
      requires |a| <= MaxSize()
      requires forall i: int {:trigger a[i]} | 0 <= i < |a| :: ValidVal(a[i])
      requires forall i: int {:trigger a[i]} | 0 <= i < |a| :: ValInGrammar(a[i], GTuple([GUint64, GUint64, GUint64, GUint64Array]))
      decreases a, s
    {
      true &&
      ValIsMap(a, MapOption(s, (x: HashMap) => x.contents))
    }
    static predicate ValIsMap(a: seq<V>, s: Option<map<uint64, Entry>>)
      requires |a| <= MaxSize()
      requires forall i: int {:trigger a[i]} | 0 <= i < |a| :: ValidVal(a[i])
      requires forall i: int {:trigger a[i]} | 0 <= i < |a| :: ValInGrammar(a[i], GTuple([GUint64, GUint64, GUint64, GUint64Array]))
      decreases a, s
    {
      (s.Some? ==>
        |s.value| as int == |a|) &&
      (s.Some? ==>
        forall v: Entry {:trigger v.loc} {:trigger v in s.value.Values} | v in s.value.Values :: 
          v.loc.Some? &&
          ValidNodeLocation(v.loc.value)) &&
      (s.Some? ==>
        forall ref: uint64 {:trigger s.value[ref]} {:trigger ref in s.value} | ref in s.value :: 
          s.value[ref].predCount == 0) &&
      (s.Some? ==>
        forall ref: uint64 {:trigger s.value[ref]} {:trigger ref in s.value} | ref in s.value :: 
          |s.value[ref].succs| <= MaxNumChildren()) &&
      (s.Some? ==>
        Marshalling.valToIndirectionTableMaps(a).Some?) &&
      (s.Some? ==>
        ghost var left: IndirectionTable := Marshalling.valToIndirectionTableMaps(a).value; ghost var right: SectorType.IndirectionTable := IMapAsIndirectionTable(s.value); left.graph == right.graph && left.locs == right.locs) &&
      (s.None? ==>
        Marshalling.valToIndirectionTableMaps(a).None?)
    }
    static lemma lemma_valToHashMapNonePrefix(a: seq<V>, i: int)
      requires forall i: int {:trigger a[i]} | 0 <= i < |a| :: ValidVal(a[i])
      requires forall i: int {:trigger a[i]} | 0 <= i < |a| :: ValInGrammar(a[i], GTuple([GUint64, GUint64, GUint64, GUint64Array]))
      requires 0 <= i <= |a| <= MaxSize()
      requires ValIsHashMap(a[..i], None)
      ensures ValIsHashMap(a, None)
      decreases |a| - i
    {
      if i == |a| {
        assert a[..i] == a;
      } else {
        assert ValIsHashMap(a[..i + 1], None) by {
          assert DropLast(a[..i + 1]) == a[..i];
          assert Last(a[..i + 1]) == a[i];
        }
        lemma_valToHashMapNonePrefix(a, i + 1);
      }
    }
    static method {:fuel ValInGrammar, 3} ValToHashMap(a: seq<V>) returns (s: lOption<HashMap>)
      requires |a| <= MaxSize()
      requires forall i: int {:trigger a[i]} | 0 <= i < |a| :: ValidVal(a[i])
      requires forall i: int {:trigger a[i]} | 0 <= i < |a| :: ValInGrammar(a[i], GTuple([GUint64, GUint64, GUint64, GUint64Array]))
      ensures ValIsHashMap(a, s.Option())
      ensures s.lSome? ==> s.value.Inv()
      ensures s.lSome? ==> s.value.count as nat < 18446744073709551616 / 8
      decreases a
    {
      var i: uint64 := 0;
      var success: bool := true;
      var mutMap: LinearHashMap<Entry> := LinearMutableMap.Constructor<Entry>(1024);
      assert Locs(mutMap) == map[];
      assert Graph(mutMap) == map[];
      while i < |a| as uint64
        invariant 0 <= i as int <= |a|
        invariant mutMap.Inv()
        invariant ValIsHashMap(a[..i], Some(mutMap))
        decreases |a| as uint64 as int - i as int
      {
        var tuple: V := a[i];
        var ref: uint64 := tuple.t[0 as uint64].u;
        var addr: uint64 := tuple.t[1 as uint64].u;
        var len: uint64 := tuple.t[2 as uint64].u;
        var succs: seq<uint64> := tuple.t[3 as uint64].ua;
        var graphRef: Option<Entry> := LinearMutableMap.Get(mutMap, ref);
        var loc: Location := Location(addr, len);
        assert ValidVal(tuple);
        assert ValidVal(tuple.t[3]);
        assert |succs| < 18446744073709551616;
        assert DropLast(a[..i + 1]) == a[..i];
        assert Last(a[..i + 1]) == a[i];
        if graphRef.Some? || !ValidNodeLocation(loc) || |succs| as uint64 > MaxNumChildrenUint64() {
          lemma_valToHashMapNonePrefix(a, (i + 1) as int);
          success := false;
          break;
        } else {
          ghost var mutMapBeforeInsert: LinearHashMap<Entry> := mutMap;
          var _inout_tmp_0: LinearHashMap<Entry>;
          _inout_tmp_0 := LinearMutableMap.Insert(inout mutMap, ref, Entry(Some(loc), succs, 0));
          mutMap := _inout_tmp_0;
          assert Locs(mutMap) == Locs(mutMapBeforeInsert)[ref := loc];
          assert Graph(mutMap) == Graph(mutMapBeforeInsert)[ref := succs];
          i := i + 1;
        }
      }
      if success {
        assert a[..i] == a;
        s := lSome(mutMap);
      } else {
        LinearMutableMap.Destructor(mutMap);
        s := lNone;
      }
    }
    static lemma lemma_count_eq_graph_size(t: HashMap)
      requires LinearMutableMap.Inv(t)
      ensures t.count as int == |Graph(t)|
      decreases t
    {
      assert Graph(t).Keys == t.contents.Keys;
      assert |Graph(t)| == |Graph(t).Keys| == |t.contents.Keys| == t.count as int;
    }
    static lemma RevealComputeRefCountsSharedDomainInv(tbl': HashMap, tbl: HashMap)
      requires ComputeRefCountsSharedInv(tbl', tbl)
      ensures forall ref: uint64 {:trigger ref in tbl'.contents} {:trigger ref in tbl.contents} | ref in tbl.contents :: ref in tbl'.contents
      ensures forall ref: uint64 {:trigger ref in tbl.contents} {:trigger ref in tbl'.contents} | ref in tbl'.contents :: ref in tbl.contents
      decreases tbl', tbl
    {
      reveal_ComputeRefCountsSharedInv();
    }
    static predicate {:opaque} {:fuel 0, 0} ComputeRefCountsSharedInv(tbl': HashMap, tbl: HashMap)
      ensures ComputeRefCountsSharedInv(tbl', tbl) ==> tbl'.count as int <= MaxSize()
      decreases tbl', tbl
    {
      tbl'.count as int <= MaxSize() &&
      (forall ref: uint64 {:trigger ref in tbl'.contents} {:trigger ref in tbl.contents} | ref in tbl.contents :: 
        ref in tbl'.contents) &&
      (forall ref: uint64 {:trigger ref in tbl.contents} {:trigger ref in tbl'.contents} | ref in tbl'.contents :: 
        ref in tbl.contents) &&
      (forall ref: uint64 {:trigger tbl.contents[ref]} {:trigger tbl'.contents[ref]} {:trigger ref in tbl.contents} | ref in tbl.contents :: 
        tbl'.contents[ref].loc == tbl.contents[ref].loc) &&
      (forall ref: uint64 {:trigger tbl.contents[ref]} {:trigger tbl'.contents[ref]} {:trigger ref in tbl.contents} | ref in tbl.contents :: 
        tbl'.contents[ref].succs == tbl.contents[ref].succs) &&
      forall ref: uint64 {:trigger tbl.contents[ref]} {:trigger ref in tbl.contents} | ref in tbl.contents :: 
        |tbl.contents[ref].succs| <= MaxNumChildren()
    }
    static predicate {:opaque} {:fuel 0, 0} ComputeRefCountsOuterLoopInv0(tbl': HashMap, tbl: HashMap, it: LinearMutableMap.Iterator<Entry>)
      requires forall ref: uint64 {:trigger ref in tbl'.contents} {:trigger ref in tbl.contents} | ref in tbl.contents :: ref in tbl'.contents
      decreases tbl', tbl, it
    {
      (forall ref: uint64 {:trigger tbl'.contents[ref]} {:trigger ref in tbl'.contents} | ref in tbl'.contents :: 
        tbl'.contents[ref].predCount as int <= 281474976710656) &&
      forall ref: NativeTypes.uint64 {:trigger IsRoot(ref)} {:trigger PredecessorSetRestricted(Graph(tbl), ref, it.s)} {:trigger tbl'.contents[ref]} {:trigger ref in tbl.contents} | ref in tbl.contents :: 
        tbl'.contents[ref].predCount as int == |PredecessorSetRestricted(Graph(tbl), ref, it.s)| + IsRoot(ref)
    }
    static predicate ComputeRefCountsOuterLoopInv(tbl': HashMap, tbl: HashMap, it: LinearMutableMap.Iterator<Entry>)
      decreases tbl', tbl, it
    {
      LinearMutableMap.Inv(tbl') &&
      LinearMutableMap.Inv(tbl) &&
      LinearMutableMap.WFIter(tbl, it) &&
      BT.G.Root() in tbl'.contents &&
      ComputeRefCountsSharedInv(tbl', tbl) &&
      (RevealComputeRefCountsSharedDomainInv(tbl', tbl); ComputeRefCountsOuterLoopInv0(tbl', tbl, it)) &&
      GraphClosedRestricted(Graph(tbl), it.s) &&
      tbl'.count as int <= MaxSize() &&
      tbl'.count == tbl.count
    }
    static predicate {:opaque} {:fuel 0, 0} ComputeRefCountsInnerLoopInv0(tbl': HashMap, tbl: HashMap, it: LinearMutableMap.Iterator<Entry>, succs: seq<BT.G.Reference>, i: uint64)
      requires it.next.Next?
      requires ComputeRefCountsSharedInv(tbl', tbl)
      decreases tbl', tbl, it, succs, i
    {
      (forall ref: uint64 {:trigger tbl'.contents[ref]} {:trigger ref in tbl'.contents} | ref in tbl'.contents :: 
        tbl'.contents[ref].predCount as int <= 281474976710656 + i as int) &&
      (RevealComputeRefCountsSharedDomainInv(tbl', tbl); forall ref: uint64 {:trigger tbl.contents[ref]} {:trigger tbl'.contents[ref]} {:trigger ref in tbl'.contents} | ref in tbl'.contents :: tbl'.contents[ref].loc == tbl.contents[ref].loc) &&
      forall ref: NativeTypes.uint64 {:trigger IsRoot(ref)} {:trigger tbl'.contents[ref]} {:trigger ref in tbl'.contents} | ref in tbl'.contents :: 
        tbl'.contents[ref].predCount as int == |PredecessorSetRestrictedPartial(Graph(tbl), ref, it.s, it.next.key, i as int)| + IsRoot(ref)
    }
    static predicate ComputeRefCountsInnerLoopInv(tbl': HashMap, tbl: HashMap, it: LinearMutableMap.Iterator<Entry>, succs: seq<BT.G.Reference>, i: uint64)
      requires it.next.Next?
      decreases tbl', tbl, it, succs, i
    {
      LinearMutableMap.Inv(tbl') &&
      LinearMutableMap.Inv(tbl) &&
      LinearMutableMap.WFIter(tbl, it) &&
      0 <= i as int <= |succs| &&
      |succs| <= MaxNumChildren() &&
      tbl'.count == tbl.count &&
      BT.G.Root() in tbl'.contents &&
      ComputeRefCountsSharedInv(tbl', tbl) &&
      (RevealComputeRefCountsSharedDomainInv(tbl', tbl); ComputeRefCountsInnerLoopInv0(tbl', tbl, it, succs, i)) &&
      succs == Graph(tbl)[it.next.key] &&
      GraphClosedRestrictedPartial(Graph(tbl), it.s, it.next.key, i as int)
    }
    static lemma ComputeRefCountsOuterLoopInvImpliesInnerLoopInv(tbl': HashMap, tbl: HashMap, it: LinearMutableMap.Iterator<Entry>, succs: seq<BT.G.Reference>)
      requires it.next.Next?
      requires succs == it.next.value.succs
      requires ComputeRefCountsOuterLoopInv(tbl', tbl, it)
      ensures ComputeRefCountsInnerLoopInv(tbl', tbl, it, succs, 0)
      decreases tbl', tbl, it, succs
    {
      reveal_ComputeRefCountsSharedInv();
      reveal_ComputeRefCountsInnerLoopInv0();
      reveal_ComputeRefCountsOuterLoopInv0();
      forall ref: NativeTypes.uint64 {:trigger IsRoot(ref)} {:trigger PredecessorSetRestrictedPartial(Graph(tbl), ref, it.s, it.next.key, 0)} {:trigger tbl'.contents[ref]} {:trigger ref in tbl'.contents} | ref in tbl'.contents
        ensures tbl'.contents[ref].predCount as int == |PredecessorSetRestrictedPartial(Graph(tbl), ref, it.s, it.next.key, 0)| + IsRoot(ref)
      {
        assert PredecessorSetRestricted(Graph(tbl), ref, it.s) == PredecessorSetRestrictedPartial(Graph(tbl), ref, it.s, it.next.key, 0);
      }
    }
    static lemma ComputeRefCountsInnerLoopInvImpliesOuterLoopInv(tbl': HashMap, tbl: HashMap, it: LinearMutableMap.Iterator<Entry>, it': LinearMutableMap.Iterator<Entry>, succs: seq<BT.G.Reference>, i: uint64)
      requires it.next.Next?
      requires succs == it.next.value.succs
      requires LinearMutableMap.WFIter(tbl, it')
      requires it'.s == it.s + {it.next.key}
      requires it'.decreaser < it.decreaser
      requires it'.next.Done? ==> it'.s == tbl.contents.Keys
      requires i as int == |succs|
      requires ComputeRefCountsInnerLoopInv(tbl', tbl, it, succs, i)
      requires BT.G.Root() in tbl'.contents
      ensures ComputeRefCountsOuterLoopInv(tbl', tbl, it')
      decreases tbl', tbl, it, it', succs, i
    {
      RevealComputeRefCountsSharedDomainInv(tbl', tbl);
      forall ref: NativeTypes.uint64 {:trigger IsRoot(ref)} {:trigger PredecessorSetRestricted(Graph(tbl), ref, it'.s)} {:trigger tbl'.contents[ref]} {:trigger ref in tbl.contents} | ref in tbl.contents
        ensures tbl'.contents[ref].predCount as int == |PredecessorSetRestricted(Graph(tbl), ref, it'.s)| + IsRoot(ref)
      {
        reveal_ComputeRefCountsInnerLoopInv0();
        assert PredecessorSetRestricted(Graph(tbl), ref, it.s + {it.next.key}) == PredecessorSetRestrictedPartial(Graph(tbl), ref, it.s, it.next.key, i as int);
      }
      assert ComputeRefCountsOuterLoopInv0(tbl', tbl, it') by {
        reveal_ComputeRefCountsOuterLoopInv0();
        forall ref: NativeTypes.uint64 {:trigger tbl'.contents[ref]} {:trigger ref in tbl'.contents} | ref in tbl'.contents
          ensures tbl'.contents[ref].predCount as int <= 281474976710656
        {
          lemma_count_eq_graph_size(tbl);
          assert forall ref: NativeTypes.uint64 {:trigger Graph(tbl)[ref]} {:trigger ref in Graph(tbl)} | ref in Graph(tbl) :: |Graph(tbl)[ref]| <= MaxNumChildren() by {
            reveal_ComputeRefCountsSharedInv();
          }
          PredecessorSetRestrictedSizeBound(Graph(tbl), ref, it'.s);
        }
      }
    }
    static lemma LemmaPredecessorSetRestrictedPartialAdd1Self(graph: map<BT.G.Reference, seq<BT.G.Reference>>, dest: BT.G.Reference, domain: set<BT.G.Reference>, next: BT.G.Reference, j: int)
      requires next in graph
      requires 0 <= j < |graph[next]|
      requires dest == graph[next][j]
      requires next !in domain
      ensures |PredecessorSetRestrictedPartial(graph, dest, domain, next, j + 1)| == |PredecessorSetRestrictedPartial(graph, dest, domain, next, j)| + 1
      decreases graph, dest, domain, next, j
    {
      assert PredecessorSetRestrictedPartial(graph, dest, domain, next, j + 1) == PredecessorSetRestrictedPartial(graph, dest, domain, next, j) + {PredecessorEdge(next, j)};
    }
    static lemma LemmaPredecessorSetRestrictedPartialAdd1Other(graph: map<BT.G.Reference, seq<BT.G.Reference>>, dest: BT.G.Reference, domain: set<BT.G.Reference>, next: BT.G.Reference, j: int)
      requires next in graph
      requires 0 <= j < |graph[next]|
      requires dest != graph[next][j]
      ensures |PredecessorSetRestrictedPartial(graph, dest, domain, next, j + 1)| == |PredecessorSetRestrictedPartial(graph, dest, domain, next, j)|
      decreases graph, dest, domain, next, j
    {
      assert PredecessorSetRestrictedPartial(graph, dest, domain, next, j + 1) == PredecessorSetRestrictedPartial(graph, dest, domain, next, j);
    }
    static method {:timeLimitMultiplier 2}  ComputeRefCountsInnerLoop(inout old_tbl': HashMap, tbl: HashMap, it: LinearMutableMap.Iterator<Entry>)
        returns (success: bool, it': LinearMutableMap.Iterator<Entry>, tbl': HashMap)
      requires it.next.Next?
      requires ComputeRefCountsOuterLoopInv(old_tbl', tbl, it)
      ensures LinearMutableMap.Inv(tbl')
      ensures success ==> ComputeRefCountsOuterLoopInv(tbl', tbl, it') && BT.G.Root() in tbl'.contents
      ensures success ==> it'.decreaser < it.decreaser
      ensures LinearMutableMap.WFIter(tbl, it')
      ensures !success ==> !BC.GraphClosed(Graph(tbl))
      decreases old_tbl', tbl, it
    {
      tbl' := old_tbl';
      var succs: seq<BT.G.Reference> := it.next.value.succs;
      var i: uint64 := 0;
      assert |succs| <= MaxNumChildren() by {
        reveal_ComputeRefCountsSharedInv();
      }
      ComputeRefCountsOuterLoopInvImpliesInnerLoopInv(tbl', tbl, it, succs);
      success := true;
      while i < |succs| as uint64
        invariant i as int <= |succs|
        invariant LinearMutableMap.Inv(tbl)
        invariant LinearMutableMap.WFIter(tbl, it)
        invariant BT.G.Root() in tbl'.contents
        invariant ComputeRefCountsInnerLoopInv(tbl', tbl, it, succs, i)
        decreases |succs| - i as int
      {
        var ref: NativeTypes.uint64 := succs[i];
        var oldEntry: Option<Entry> := LinearMutableMap.Get(tbl', ref);
        if oldEntry.Some? {
          assert Graph(tbl)[it.next.key][i] in Graph(tbl) by {
            RevealComputeRefCountsSharedDomainInv(tbl', tbl);
          }
          reveal_ComputeRefCountsInnerLoopInv0();
          var newEntry: Entry := oldEntry.value.(predCount := oldEntry.value.predCount + 1);
          var _inout_tmp_0: LinearHashMap<Entry>;
          _inout_tmp_0 := LinearMutableMap.Insert(inout tbl', ref, newEntry);
          tbl' := _inout_tmp_0;
          forall r: NativeTypes.uint64 {:trigger IsRoot(r)} {:trigger tbl'.contents[r]} {:trigger r in tbl'.contents} | r in tbl'.contents
            ensures tbl'.contents[r].predCount as int == |PredecessorSetRestrictedPartial(Graph(tbl), r, it.s, it.next.key, (i + 1) as int)| + IsRoot(r)
          {
            if r == ref {
              LemmaPredecessorSetRestrictedPartialAdd1Self(Graph(tbl), r, it.s, it.next.key, i as int);
            } else {
              LemmaPredecessorSetRestrictedPartialAdd1Other(Graph(tbl), r, it.s, it.next.key, i as int);
            }
          }
          i := i + 1;
          assert ComputeRefCountsInnerLoopInv(tbl', tbl, it, succs, i) by {
            assert ComputeRefCountsSharedInv(tbl', tbl) by {
              reveal_ComputeRefCountsSharedInv();
            }
            assert ComputeRefCountsInnerLoopInv0(tbl', tbl, it, succs, i);
          }
        } else {
          assert tbl'.contents.Keys == tbl.contents.Keys by {
            reveal_ComputeRefCountsSharedInv();
          }
          assert ref in Graph(tbl)[it.next.key];
          success := false;
          break;
        }
      }
      it' := LinearMutableMap.IterInc(tbl, it);
      if success {
        ComputeRefCountsInnerLoopInvImpliesOuterLoopInv(tbl', tbl, it, it', succs, i);
      }
    }
    static method ComputeRefCounts(tbl: HashMap) returns (tbl': lOption<HashMap>)
      requires tbl.Inv()
      requires forall ref: uint64 {:trigger tbl.contents[ref]} {:trigger ref in tbl.contents} | ref in tbl.contents :: tbl.contents[ref].predCount == 0
      requires forall ref: uint64 {:trigger tbl.contents[ref]} {:trigger ref in tbl.contents} | ref in tbl.contents :: |tbl.contents[ref].succs| <= MaxNumChildren()
      requires tbl.count as int <= MaxSize()
      requires BT.G.Root() in tbl.contents
      ensures tbl'.lSome? ==> tbl'.value.Inv() && |tbl'.value.contents| <= 4294967296
      ensures BC.GraphClosed(Graph(tbl)) <==> tbl'.lSome?
      ensures tbl'.lSome? ==> Graph(tbl) == Graph(tbl'.value)
      ensures tbl'.lSome? ==> Locs(tbl) == Locs(tbl'.value)
      ensures tbl'.lSome? ==> ValidPredCounts(PredCounts(tbl'.value), Graph(tbl'.value))
      ensures tbl'.lSome? ==> BT.G.Root() in tbl'.value.contents
      decreases tbl
    {
      var t1: LinearHashMap<Entry> := LinearMutableMap.Clone(tbl);
      var oldEntryOpt: Option<Entry> := LinearMutableMap.Get(t1, BT.G.Root());
      var oldEntry: Entry := oldEntryOpt.value;
      var _inout_tmp_1: LinearHashMap<Entry>;
      _inout_tmp_1 := LinearMutableMap.Insert(inout t1, BT.G.Root(), oldEntry.(predCount := 1));
      t1 := _inout_tmp_1;
      var it: Iterator<Entry> := LinearMutableMap.IterStart(tbl);
      assert ComputeRefCountsOuterLoopInv(t1, tbl, it) by {
        forall ref: NativeTypes.uint64 {:trigger IsRoot(ref)} {:trigger PredecessorSetRestricted(Graph(t1), ref, it.s)} {:trigger t1.contents[ref]} {:trigger ref in t1.contents} | ref in t1.contents
          ensures t1.contents[ref].predCount as int == |PredecessorSetRestricted(Graph(t1), ref, it.s)| + IsRoot(ref)
        {
          assert PredecessorSetRestricted(Graph(t1), ref, it.s) == {};
        }
        reveal_ComputeRefCountsSharedInv();
        reveal_ComputeRefCountsOuterLoopInv0();
      }
      var success: bool := true;
      while it.next.Next?
        invariant ComputeRefCountsOuterLoopInv(t1, tbl, it)
        decreases it.decreaser
      {
        var _inout_tmp_0: HashMap;
        success, it, _inout_tmp_0 := ComputeRefCountsInnerLoop(inout t1, tbl, it);
        t1 := _inout_tmp_0;
        if !success {
          break;
        }
        assert ComputeRefCountsOuterLoopInv(t1, tbl, it);
      }
      if success {
        tbl' := lSome(t1);
        assert Graph(tbl) == Graph(tbl'.value) && Locs(tbl) == Locs(tbl'.value) by {
          reveal_ComputeRefCountsSharedInv();
        }
        assert ValidPredCounts(PredCounts(tbl'.value), Graph(tbl'.value)) by {
          reveal_ComputeRefCountsSharedInv();
          reveal_ComputeRefCountsOuterLoopInv0();
          reveal_PredCounts();
          forall ref: NativeTypes.uint64 {:trigger IsRoot(ref)} {:trigger PredecessorSet(Graph(tbl'.value), ref)} {:trigger PredCounts(tbl'.value)[ref]} {:trigger ref in PredCounts(tbl'.value)} | ref in PredCounts(tbl'.value)
            ensures PredCounts(tbl'.value)[ref] == |PredecessorSet(Graph(tbl'.value), ref)| + IsRoot(ref)
          {
            assert PredecessorSetRestricted(Graph(tbl'.value), ref, it.s) == PredecessorSet(Graph(tbl'.value), ref);
          }
        }
        assert BC.GraphClosed(Graph(tbl));
      } else {
        LinearMutableMap.Destructor(t1);
        tbl' := lNone;
        assert !BC.GraphClosed(Graph(tbl));
      }
    }
    static method MakeGarbageQueue(t: HashMap) returns (q: USeq.USeq)
      requires t.Inv()
      requires |t.contents| <= 4294967296
      ensures q.Inv()
      ensures forall ref: uint64 {:trigger ref in q.I()} {:trigger t.contents[ref]} {:trigger ref in t.contents} | ref in t.contents && t.contents[ref].predCount == 0 :: ref in q.I()
      ensures forall ref: uint64 {:trigger t.contents[ref]} {:trigger ref in t.contents} {:trigger ref in q.I()} | ref in q.I() :: ref in t.contents && t.contents[ref].predCount == 0
      decreases t
    {
      q := USeq.USeq.Alloc();
      var it: Iterator<Entry> := LinearMutableMap.IterStart(t);
      while it.next.Next?
        invariant q.Inv()
        invariant LinearMutableMap.Inv(t)
        invariant LinearMutableMap.WFIter(t, it)
        invariant Set(q.I()) <= t.contents.Keys
        invariant |t.contents| <= 4294967296
        invariant forall ref: uint64 {:trigger ref in q.I()} {:trigger ref in it.s} {:trigger t.contents[ref]} {:trigger ref in t.contents} | ref in t.contents && t.contents[ref].predCount == 0 && ref in it.s :: ref in q.I()
        invariant forall ref: uint64 {:trigger ref in it.s} {:trigger t.contents[ref]} {:trigger ref in t.contents} {:trigger ref in q.I()} | ref in q.I() :: ref in t.contents && t.contents[ref].predCount == 0 && ref in it.s
        decreases it.decreaser
      {
        if it.next.value.predCount == 0 {
          NoDupesSetCardinality(q.I());
          SetInclusionImpliesSmallerCardinality(Set(q.I()), t.contents.Keys);
          assert |t.contents.Keys| == |t.contents|;
          var _inout_tmp_0: USeq;
          _inout_tmp_0 := q.Add(inout q, it.next.key);
          q := _inout_tmp_0;
        }
        it := LinearMutableMap.IterInc(t, it);
      }
    }
    static method ComputeRefUpperBound(t: HashMap) returns (r: uint64)
      requires t.Inv()
      ensures forall ref: uint64 {:trigger ref in t.contents} | ref in t.contents :: ref <= r
      decreases t
    {
      var it: Iterator<Entry> := LinearMutableMap.IterStart(t);
      var refUpperBound: uint64 := 0;
      while it.next.Next?
        invariant LinearMutableMap.Inv(t)
        invariant LinearMutableMap.WFIter(t, it)
        invariant forall ref: uint64 {:trigger ref in it.s} | ref in it.s :: ref <= refUpperBound
        decreases it.decreaser
      {
        if it.next.key > refUpperBound {
          refUpperBound := it.next.key;
        }
        it := LinearMutableMap.IterInc(t, it);
      }
      r := refUpperBound;
    }
    static method ValToIndirectionTable(v: V) returns (s: lOption<IndirectionTable>)
      requires ValidVal(v)
      requires ValInGrammar(v, IndirectionTableGrammar())
      ensures s.lSome? ==> s.value.Inv()
      ensures s.lSome? ==> Marshalling.valToIndirectionTable(v) == Some(s.value.I())
      ensures s.lSome? ==> s.value.TrackingGarbage()
      ensures s.lNone? ==> Marshalling.valToIndirectionTable(v).None?
      decreases v
    {
      if |v.a| as uint64 <= MaxSizeUint64() {
        var res: lOption<HashMap> := ValToHashMap(v.a);
        match res {
          case lSome(t) =>
            {
              var rootRef := LinearMutableMap.Get(t, BT.G.Root());
              if rootRef.Some? {
                var t1opt := ComputeRefCounts(t);
                LinearMutableMap.Destructor(t);
                match t1opt {
                  case lSome(t1) =>
                    {
                      assert t1.Inv();
                      assert |t1.contents| <= 4294967296;
                      var q := MakeGarbageQueue(t1);
                      var refUpperBound := ComputeRefUpperBound(t1);
                      s := lSome(IndirectionTable(t1, lSome(q), refUpperBound, None, Locs(t1), Graph(t1), PredCounts(t1)));
                      assert s.lSome? ==> s.value.Inv() by {
                        reveal s.value.Inv();
                      }
                    }
                  case lNone =>
                    {
                      s := lNone;
                    }
                }
              } else {
                LinearMutableMap.Destructor(t);
                s := lNone;
              }
            }
          case lNone() =>
            {
              s := lNone;
            }
        }
      } else {
        s := lNone;
      }
    }
    static function MaxIndirectionTableByteSize(): int
    {
      8 + MaxSize() * (8 + 8 + 8 + 8 + MaxNumChildren() * 8)
    }
    static lemma lemma_SeqSum_prefix_array(a: array<V>, i: int)
      requires 0 < i <= a.Length
      ensures SeqSum(a[..i - 1]) + SizeOfV(a[i - 1]) == SeqSum(a[..i])
      decreases a, i
    {
      lemma_SeqSum_prefix(a[..i - 1], a[i - 1]);
      assert a[..i - 1] + [a[i - 1]] == a[..i];
    }
    static lemma {:fuel SizeOfV, 5} lemma_tuple_size(a: uint64, b: uint64, c: uint64, succs: seq<BT.G.Reference>)
      requires |succs| <= MaxNumChildren()
      ensures SizeOfV(VTuple([VUint64(a), VUint64(b), VUint64(c), VUint64Array(succs)])) == 8 + 8 + 8 + 8 + |succs| * 8
      decreases a, b, c, succs
    {
    }
    static lemma lemma_SeqSum_empty()
      ensures SeqSum([]) == 0
    {
      reveal_SeqSum();
    }
    static function IMapAsIndirectionTable(m: map<uint64, Entry>): SectorType.IndirectionTable
      decreases m
    {
      SectorType.IndirectionTable(MapLocs(m), MapGraph(m))
    }
    static function method IndirectionTableGrammar(): G
      ensures ValidGrammar(IndirectionTableGrammar())
    {
      GArray(GTuple([GUint64, GUint64, GUint64, GUint64Array]))
    }
    method {:timeLimitMultiplier 3}  IndirectionTableToVal() returns (v: V, size: uint64)
      requires this.Inv()
      requires BC.WFCompleteIndirectionTable(this.I())
      ensures ValInGrammar(v, IndirectionTableGrammar())
      ensures ValidVal(v)
      ensures Marshalling.valToIndirectionTable(v).Some?
      ensures Marshalling.valToIndirectionTable(v).value == this.I()
      ensures SizeOfV(v) <= MaxIndirectionTableByteSize()
      ensures SizeOfV(v) == size as int
      decreases this
    {
      reveal Inv();
      assert this.t.count <= MaxSizeUint64();
      lemma_SeqSum_empty();
      var count: uint64 := this.t.count as uint64;
      var a: array<V> := new V[count];
      var it: Iterator<Entry> := LinearMutableMap.IterStart(this.t);
      var i: uint64 := 0;
      size := 0;
      ghost var partial: map<uint64, Entry> := map[];
      assert MapLocs(map[]) == map[];
      assert MapGraph(map[]) == map[];
      assert ValIsMap(a[..i], Some(partial));
      while it.next.Next?
        invariant this.Inv()
        invariant BC.WFCompleteIndirectionTable(this.I())
        invariant 0 <= i as int <= a.Length
        invariant LinearMutableMap.WFIter(this.t, it)
        invariant forall j: uint64 {:trigger a[j]} | 0 <= j < i :: ValidVal(a[j])
        invariant forall j: uint64 {:trigger a[j]} | 0 <= j < i :: ValInGrammar(a[j], GTuple([GUint64, GUint64, GUint64, GUint64Array]))
        invariant |partial.Keys| == i as nat
        invariant partial.Keys == it.s
        invariant partial.Keys <= this.t.contents.Keys
        invariant ValIsMap(a[..i], Some(partial))
        invariant forall r: uint64 {:trigger this.t.contents[r]} {:trigger partial[r]} {:trigger r in this.t.contents} {:trigger r in partial} | r in partial :: r in this.t.contents && partial[r].loc == this.t.contents[r].loc && partial[r].succs == this.t.contents[r].succs
        invariant size as int <= |it.s| * (8 + 8 + 8 + 8 + MaxNumChildren() * 8)
        invariant SeqSum(a[..i]) == size as int
        decreases it.decreaser
      {
        var (ref: uint64, locOptGraph: Entry) := (it.next.key, it.next.value);
        assert ref in this.I().locs;
        var locOpt: Option<Location> := locOptGraph.loc;
        var succs: seq<BT.G.Reference> := locOptGraph.succs;
        var loc: Location := locOpt.value;
        var childrenVal: V := VUint64Array(succs);
        assert |succs| <= MaxNumChildren();
        assert ValidNodeLocation(loc);
        LinearMutableMap.LemmaIterIndexLtCount(this.t, it);
        assert |succs| < 18446744073709551616;
        assert ValidVal(VTuple([VUint64(ref), VUint64(loc.addr), VUint64(loc.len), childrenVal]));
        var vi: V := VTuple([VUint64(ref), VUint64(loc.addr), VUint64(loc.len), childrenVal]);
        ghost var partialBefore: map<uint64, Entry> := partial;
        ghost var itBeforeInc: Iterator<Entry> := it;
        partial := partial[ref := Entry(locOpt, succs, 0)];
        a[i] := vi;
        i := i + 1;
        it := LinearMutableMap.IterInc(this.t, it);
        assert |itBeforeInc.s| + 1 == |it.s|;
        assert a[..i - 1] == DropLast(a[..i]);
        calc == {
          SeqSum(a[..i]);
        ==
          {
            lemma_SeqSum_prefix_array(a, i as int);
          }
          SeqSum(a[..i - 1]) + SizeOfV(a[i - 1]);
        ==
          SeqSum(a[..i - 1]) + SizeOfV(vi);
        ==
          {
            lemma_tuple_size(ref, loc.addr, loc.len, succs);
          }
          size as int + 8 + 8 + 8 + 8 + 8 * |succs|;
        }
        size := size + 32 + 8 * |succs| as uint64;
        assert ValIsMap(a[..i], Some(partial)) by {
          ghost var aBefore: Option<IndirectionTable> := Marshalling.valToIndirectionTableMaps(DropLast(a[..i]));
          assert aBefore.value.locs[ref := loc] == MapLocs(partial);
          assert aBefore.value.graph[ref := succs] == MapGraph(partial);
        }
      }
      SetInclusionAndEqualCardinalityImpliesSetEquality(partial.Keys, this.t.contents.Keys);
      assert a[..i] == a[..];
      v := VArray(a[..]);
      assert |it.s| <= MaxSize();
      size := size + 8;
      assert Marshalling.valToIndirectionTable(v).Some?;
    }
    static predicate IsLocAllocBitmap(bm: BitmapModel.BitmapModelT, i: int)
      decreases bm, i
    {
      0 <= i < BitmapModel.Len(bm) &&
      BitmapModel.IsSet(bm, i)
    }
    static method BitmapInitUpTo(inout old_bm: BitmapImpl.Bitmap, upTo: uint64) returns (bm: BitmapImpl.Bitmap)
      requires old_bm.Inv()
      requires upTo as int <= BitmapModel.Len(old_bm.I())
      ensures bm.Inv()
      ensures BitmapModel.Len(old_bm.I()) == BitmapModel.Len(bm.I())
      ensures forall j: nat | j < BitmapModel.Len(old_bm.I()) :: (j < upTo as nat ==> BitmapModel.IsSet(bm.I(), j as int)) && (j >= upTo as nat ==> BitmapModel.IsSet(bm.I(), j as int) == BitmapModel.IsSet(old_bm.I(), j as int))
      decreases old_bm, upTo
    {
      bm := old_bm;
      var i: uint64 := 0;
      while i < upTo
        invariant i <= upTo
        invariant bm.Inv()
        invariant upTo as int <= BitmapModel.Len(bm.I())
        invariant BitmapModel.Len(old_bm.I()) == BitmapModel.Len(bm.I())
        invariant forall j: nat | j < BitmapModel.Len(old_bm.I()) :: (j < i as nat ==> BitmapModel.IsSet(bm.I(), j as int)) && (j >= i as nat ==> BitmapModel.IsSet(bm.I(), j as int) == BitmapModel.IsSet(old_bm.I(), j as int))
        decreases upTo as int - i as int
      {
        var _inout_tmp_0: Bitmap;
        _inout_tmp_0 := bm.Set(inout bm, i);
        bm := _inout_tmp_0;
        i := i + 1;
        BitmapModel.reveal_BitSet();
        BitmapModel.reveal_IsSet();
      }
    }
    predicate IsLocAllocIndirectionTablePartial(i: int, s: set<uint64>)
      decreases this, i, s
    {
      0 <= i < MinNodeBlockIndex() || !forall ref: NativeTypes.uint64 {:trigger this.locs[ref]} {:trigger ref in s} {:trigger ref in this.locs} | ref in this.locs && ref in s :: this.locs[ref].addr as int != i * NodeBlockSize() as int
    }
    method InitLocBitmap() returns (success: bool, bm: BitmapImpl.Bitmap)
      requires this.Inv()
      requires BC.WFCompleteIndirectionTable(this.I())
      ensures bm.Inv()
      ensures BitmapModel.Len(bm.I()) == NumBlocks()
      ensures success ==> (forall i: nat {:trigger IsLocAllocBitmap(bm.I(), i)} {:trigger I().IsLocAllocIndirectionTable(i)} :: I().IsLocAllocIndirectionTable(i) <==> IsLocAllocBitmap(bm.I(), i)) && BC.AllLocationsForDifferentRefsDontOverlap(I())
      decreases this
    {
      reveal this.Inv();
      bm := BitmapImpl.Bitmap.Constructor(NumBlocksUint64());
      assert BitmapModel.Len(bm.I()) == NumBlocks();
      MinNodeBlockIndex_le_NumBlocks();
      var _inout_tmp_1: BitmapImpl.Bitmap;
      _inout_tmp_1 := BitmapInitUpTo(inout bm, MinNodeBlockIndexUint64());
      bm := _inout_tmp_1;
      var it: Iterator<Entry> := LinearMutableMap.IterStart(this.t);
      assert BitmapModel.Len(bm.I()) == NumBlocks();
      assert NumBlocks() == NumBlocksUint64() as int;
      success := true;
      forall i: nat {:trigger IsLocAllocBitmap(bm.I(), i)} {:trigger IsLocAllocIndirectionTablePartial(i, it.s)} | true
        ensures IsLocAllocIndirectionTablePartial(i, it.s) <==> IsLocAllocBitmap(bm.I(), i)
      {
        assert IsLocAllocIndirectionTablePartial(i, it.s) ==> 0 <= i < BitmapModel.Len(bm.I());
        assert IsLocAllocBitmap(bm.I(), i) ==> 0 <= i < BitmapModel.Len(bm.I());
      }
      forall r: uint64 {:trigger r in it.s} {:trigger this.t.contents[r]} {:trigger r in this.I().locs} | r in this.I().locs
        ensures true && var li: int := this.t.contents[r].loc.value.addr as int / NodeBlockSize(); MinNodeBlockIndex() <= li && (li < NumBlocks() ==> (r in it.s <==> BitmapModel.IsSet(bm.I(), li)))
      {
        assert ValidNodeLocation(this.t.contents[r].loc.value);
        assert ValidNodeAddr(this.t.contents[r].loc.value.addr);
        ghost var li: int := ValidNodeAddrDivisor(this.t.contents[r].loc.value.addr);
        Math.lemma_mul_invert(this.t.contents[r].loc.value.addr as int, NodeBlockSize(), li);
        assert li == this.t.contents[r].loc.value.addr as int / NodeBlockSize();
        assert MinNodeBlockIndex() <= li;
        if li < NumBlocks() {
          if r in it.s {
            assert false;
          } else {
            assert !BitmapModel.IsSet(bm.I(), li);
          }
        }
      }
      while it.next.Next?
        invariant this.t.Inv()
        invariant BC.WFCompleteIndirectionTable(this.I())
        invariant bm.Inv()
        invariant LinearMutableMap.WFIter(this.t, it)
        invariant BitmapModel.Len(bm.I()) == NumBlocks()
        invariant forall i: nat {:trigger IsLocAllocBitmap(bm.I(), i)} {:trigger IsLocAllocIndirectionTablePartial(i, it.s)} :: IsLocAllocIndirectionTablePartial(i, it.s) <==> IsLocAllocBitmap(bm.I(), i)
        invariant success ==> forall r1: NativeTypes.uint64, r2: NativeTypes.uint64 {:trigger LocationsForDifferentRefsDontOverlap(this.I(), r1, r2)} {:trigger r2 in it.s, r1 in it.s} {:trigger r2 in it.s, r1 in this.I().locs} {:trigger r1 in it.s, r2 in this.I().locs} {:trigger r2 in this.I().locs, r1 in this.I().locs} | r1 in this.I().locs && r2 in this.I().locs :: r1 in it.s && r2 in it.s ==> LocationsForDifferentRefsDontOverlap(this.I(), r1, r2)
        decreases it.decreaser
      {
        assert it.next.key in this.I().locs;
        var loc: uint64 := it.next.value.loc.value.addr;
        assert 0 <= loc as nat / NodeBlockSize() < Uint64UpperBound() by {
          NonlinearLemmas.div_ge_0(loc as nat, NodeBlockSize());
          NonlinearLemmas.div_denom_ge_1(loc as nat, NodeBlockSize());
        }
        assert loc as int % NodeBlockSize() == 0 by {
          reveal_ValidNodeAddr();
          assert ValidNodeLocation(it.next.value.loc.value);
        }
        var locIndex: uint64 := loc / NodeBlockSizeUint64();
        if locIndex < NumBlocksUint64() {
          var isSet: bool := bm.GetIsSet(locIndex);
          if !isSet {
            ghost var it0: Iterator<Entry> := it;
            ghost var bm0: BitmapImpl.Bitmap := bm;
            it := LinearMutableMap.IterInc(this.t, it);
            var _inout_tmp_0: Bitmap;
            _inout_tmp_0 := bm.Set(inout bm, locIndex);
            bm := _inout_tmp_0;
            assert forall i: nat {:trigger IsLocAllocBitmap(bm0.I(), i)} {:trigger IsLocAllocIndirectionTablePartial(i, it0.s)} :: IsLocAllocIndirectionTablePartial(i, it0.s) <==> IsLocAllocBitmap(bm0.I(), i);
            forall i: nat {:trigger IsLocAllocBitmap(bm.I(), i)} {:trigger IsLocAllocIndirectionTablePartial(i, it.s)} | IsLocAllocIndirectionTablePartial(i, it.s)
              ensures IsLocAllocBitmap(bm.I(), i)
            {
              BitmapModel.reveal_BitSet();
              BitmapModel.reveal_IsSet();
              if i == locIndex as nat {
                assert IsLocAllocBitmap(bm.I(), i);
              } else {
                if 0 <= i < MinNodeBlockIndex() {
                  if IsLocAllocIndirectionTablePartial(i, it0.s) {
                  }
                } else {
                  assert MinNodeBlockIndex() <= i;
                  assert exists ref: NativeTypes.uint64 {:trigger this.locs[ref]} {:trigger ref in it.s} {:trigger ref in this.locs} | ref in this.locs && ref in it.s :: !(this.locs[ref].addr as int != i * NodeBlockSize() as int);
                  assert loc as int / NodeBlockSize() == locIndex as int;
                  assert loc as int / NodeBlockSize() != i;
                  assert loc as int % NodeBlockSize() == 0;
                  NonlinearLemmas.div_invert(loc as int, NodeBlockSize(), i);
                  assert loc as int != i * NodeBlockSize();
                  assert it.s == it0.s + {it0.next.key};
                  assert IsLocAllocIndirectionTablePartial(i, it0.s) by {
                    assert exists ref: NativeTypes.uint64 {:trigger this.locs[ref]} {:trigger ref in it0.s} {:trigger ref in this.locs} | ref in this.locs && ref in it0.s :: !(this.locs[ref].addr as int != i * NodeBlockSize() as int);
                  }
                }
              }
            }
            forall i: nat {:trigger IsLocAllocIndirectionTablePartial(i, it.s)} {:trigger IsLocAllocBitmap(bm.I(), i)} | IsLocAllocBitmap(bm.I(), i)
              ensures IsLocAllocIndirectionTablePartial(i, it.s)
            {
              BitmapModel.reveal_BitSet();
              BitmapModel.reveal_IsSet();
              if IsLocAllocBitmap(bm0.I(), i) {
              }
              if IsLocAllocIndirectionTablePartial(i, it0.s) {
              }
              if i == locIndex as int {
                ghost var ref: uint64 := it0.next.key;
                assert this.t.contents[ref].loc.Some?;
                assert ref in it.s;
                assert this.t.contents[ref] == it0.next.value;
                assert this.t.contents[ref].loc.value.addr as int / NodeBlockSize() == i;
                assert this.t.contents[ref].loc.value.addr == loc;
                assert loc as int % NodeBlockSize() == 0;
                NonlinearLemmas.div_invert(loc as int, NodeBlockSize(), i);
                assert this.t.contents[ref].loc.value.addr as int == i * NodeBlockSize() as int;
                assert IsLocAllocIndirectionTablePartial(i, it.s);
              } else {
                assert IsLocAllocIndirectionTablePartial(i, it.s);
              }
            }
            forall r1: NativeTypes.uint64, r2: NativeTypes.uint64 {:trigger LocationsForDifferentRefsDontOverlap(this.I(), r1, r2)} {:trigger r2 in it.s, r1 in it.s} {:trigger r2 in it.s, r1 in this.I().locs} {:trigger r1 in it.s, r2 in this.I().locs} {:trigger r2 in this.I().locs, r1 in this.I().locs} | r1 in this.I().locs && r2 in this.I().locs && r1 in it.s && r2 in it.s
              ensures LocationsForDifferentRefsDontOverlap(this.I(), r1, r2)
            {
              if r1 != r2 {
                if r1 in it0.s && r2 in it0.s {
                  assert BC.LocationsForDifferentRefsDontOverlap(this.I(), r1, r2);
                } else {
                  reveal_ValidNodeAddr();
                  if this.I().locs[r1].addr == this.I().locs[r2].addr {
                    assert ValidNodeLocation(this.t.contents[r1].loc.value);
                    assert ValidNodeAddr(this.t.contents[r1].loc.value.addr);
                    ghost var j1: int := DiskLayout.ValidNodeAddrDivisor(this.I().locs[r1].addr);
                    ghost var j2: int := DiskLayout.ValidNodeAddrDivisor(this.I().locs[r2].addr);
                    Math.lemma_mul_invert(this.t.contents[r1].loc.value.addr as int, NodeBlockSize(), j1);
                    Math.lemma_mul_invert(this.t.contents[r2].loc.value.addr as int, NodeBlockSize(), j2);
                    if r1 !in it0.s {
                      assert r2 in it0.s;
                      assert !BitmapModel.IsSet(bm0.I(), j1);
                      assert IsLocAllocBitmap(bm0.I(), j2);
                      assert BitmapModel.IsSet(bm0.I(), j2);
                      assert false;
                    } else {
                      assert r1 in it0.s;
                      assert !BitmapModel.IsSet(bm0.I(), j2);
                      assert IsLocAllocBitmap(bm0.I(), j1);
                      assert BitmapModel.IsSet(bm0.I(), j1);
                      assert false;
                    }
                  } else {
                    assert BC.LocationsForDifferentRefsDontOverlap(this.I(), r1, r2);
                  }
                }
              }
            }
          } else {
            success := false;
            break;
          }
        } else {
          success := false;
          break;
        }
      }
    }
    predicate deallocable(ref: BT.G.Reference)
      decreases this, ref
    {
      ref in this.I().graph &&
      ref != BT.G.Root() &&
      forall r: NativeTypes.uint64 {:trigger this.I().graph[r]} {:trigger r in this.I().graph} | r in this.I().graph :: 
        ref !in this.I().graph[r]
    }
    method FindDeallocable() returns (ref: Option<BT.G.Reference>)
      requires this.Inv()
      requires this.TrackingGarbage()
      ensures ref.Some? ==> ref.value in this.I().graph
      ensures ref.Some? ==> this.deallocable(ref.value)
      ensures ref.None? ==> forall r: NativeTypes.uint64 {:trigger this.deallocable(r)} {:trigger r in this.I().graph} | r in this.I().graph :: !this.deallocable(r)
      decreases this
    {
      reveal this.Inv();
      ref := this.garbageQueue.value.FirstOpt();
      if ref.None? {
        forall r: NativeTypes.uint64 {:trigger this.deallocable(r)} {:trigger r in this.I().graph} | r in this.I().graph
          ensures !this.deallocable(r)
        {
          assert this.t.contents[r].predCount != 0;
          if r == BT.G.Root() {
            assert !this.deallocable(r);
          } else {
            reveal_PredCounts();
            ghost var y: PredecessorEdge :| y in PredecessorSet(this.graph, r);
          }
        }
      } else {
        reveal_PredCounts();
        assert this.predCounts[ref.value] == |PredecessorSet(this.graph, ref.value)| + IsRoot(ref.value);
        forall r: NativeTypes.uint64 {:trigger this.I().graph[r]} {:trigger r in this.I().graph} | r in this.I().graph
          ensures ref.value !in this.I().graph[r]
        {
          if ref.value in this.I().graph[r] {
            ghost var i: int :| 0 <= i < |this.I().graph[r]| && this.I().graph[r][i] == ref.value;
            assert PredecessorEdge(r, i) in PredecessorSet(this.graph, ref.value);
          }
        }
      }
    }
    function method GetSize(): (size: uint64)
      requires this.Inv()
      ensures size as int == |this.I().graph|
      decreases this
    {
      reveal this.Inv();
      lemma_count_eq_graph_size(this.t);
      this.t.count
    }
    method FindRefWithNoLoc(inout old_self: IndirectionTable) returns (ref: Option<BT.G.Reference>, self: IndirectionTable)
      requires old_self.Inv()
      ensures self.Inv()
      ensures self.I() == old_self.I()
      ensures ref.Some? ==> ref.value in old_self.graph
      ensures ref.Some? ==> ref.value !in old_self.locs
      ensures ref.None? ==> forall r: NativeTypes.uint64 {:trigger r in old_self.locs} {:trigger r in old_self.graph} | r in old_self.graph :: r in old_self.locs
      decreases this, old_self
    {
      self := old_self;
      reveal old_self.Inv();
      var findLoclessIterator: Option<LinearMutableMap.SimpleIterator> := self.findLoclessIterator;
      var it: LinearMutableMap.SimpleIterator;
      if findLoclessIterator.Some? {
        it := findLoclessIterator.value;
      } else {
        it := LinearMutableMap.SimpleIterStart(self.t);
      }
      while true
        invariant self.Inv()
        invariant self == old_self.(t := self.t)
        invariant LinearMutableMap.WFSimpleIter(self.t, it)
        invariant forall r: uint64 {:trigger r in self.I().locs} {:trigger r in it.s} | r in it.s :: r in self.I().locs
        decreases it.decreaser
      {
        var next: IteratorOutput<Entry> := LinearMutableMap.SimpleIterOutput(self.t, it);
        if next.Next? {
          if next.value.loc.None? {
            var _inout_tmp_0: Option<LinearMutableMap.SimpleIterator> := Some(it);
            self := self.(findLoclessIterator := _inout_tmp_0);
            ref := Some(next.key);
            break;
          } else {
            it := LinearMutableMap.SimpleIterInc(self.t, it);
          }
        } else {
          var _inout_tmp_1: Option<LinearMutableMap.SimpleIterator> := Some(it);
          self := self.(findLoclessIterator := _inout_tmp_1);
          ref := None;
          break;
        }
      }
      reveal self.Inv();
    }
    function {:opaque} {:fuel 0, 0} getRefUpperBound(): (r: uint64)
      requires Inv()
      ensures forall ref: uint64 {:trigger ref in this.graph} | ref in this.graph :: ref <= r
      decreases this
    {
      reveal Inv();
      this.refUpperBound
    }
    method GetRefUpperBound() returns (r: uint64)
      requires this.Inv()
      ensures r == this.getRefUpperBound()
      decreases this
    {
      reveal_getRefUpperBound();
      r := this.refUpperBound;
    }
  }
  function MapLocs(t: map<uint64, Entry>): map<BT.G.Reference, Location>
    decreases t
  {
    map ref: uint64 {:trigger t[ref]} {:trigger ref in t} | ref in t && t[ref].loc.Some? :: t[ref].loc.value
  }
  function Locs(t: HashMap): map<BT.G.Reference, Location>
    decreases t
  {
    map ref: uint64 {:trigger t.contents[ref]} {:trigger ref in t.contents} | ref in t.contents && t.contents[ref].loc.Some? :: t.contents[ref].loc.value
  }
  function MapGraph(t: map<uint64, Entry>): map<BT.G.Reference, seq<BT.G.Reference>>
    decreases t
  {
    map ref: uint64 {:trigger t[ref]} {:trigger ref in t} | ref in t :: t[ref].succs
  }
  function Graph(t: HashMap): map<BT.G.Reference, seq<BT.G.Reference>>
    decreases t
  {
    map ref: uint64 {:trigger t.contents[ref]} {:trigger ref in t.contents} | ref in t.contents :: t.contents[ref].succs
  }
  function {:opaque} {:fuel 0, 0} PredCounts(t: HashMap): (m: map<BT.G.Reference, int>)
    decreases t
  {
    map ref: uint64 {:trigger t.contents[ref]} {:trigger ref in t.contents} | ref in t.contents :: t.contents[ref].predCount as int
  }
  function PredecessorSet(graph: map<BT.G.Reference, seq<BT.G.Reference>>, dest: BT.G.Reference): set<PredecessorEdge>
    decreases graph, dest
  {
    set src: NativeTypes.uint64, idx: int {:trigger PredecessorEdge(src, idx)} {:trigger graph[src][idx]} | src in graph && 0 <= idx < |graph[src]| && graph[src][idx] == dest :: PredecessorEdge(src, idx)
  }
  function PredecessorSetRestricted(graph: map<BT.G.Reference, seq<BT.G.Reference>>, dest: BT.G.Reference, domain: set<BT.G.Reference>): set<PredecessorEdge>
    decreases graph, dest, domain
  {
    set src: NativeTypes.uint64, idx: int {:trigger PredecessorEdge(src, idx)} {:trigger graph[src][idx]} | src in graph && 0 <= idx < |graph[src]| && graph[src][idx] == dest && src in domain :: PredecessorEdge(src, idx)
  }
  function PredecessorSetRestrictedPartial(graph: map<BT.G.Reference, seq<BT.G.Reference>>, dest: BT.G.Reference, domain: set<BT.G.Reference>, next: BT.G.Reference, j: int): set<PredecessorEdge>
    decreases graph, dest, domain, next, j
  {
    set src: NativeTypes.uint64, idx: int {:trigger PredecessorEdge(src, idx)} {:trigger graph[src][idx]} | src in graph && 0 <= idx < |graph[src]| && graph[src][idx] == dest && (src in domain || (src == next && idx < j)) :: PredecessorEdge(src, idx)
  }
  predicate GraphClosedRestricted(graph: map<BT.G.Reference, seq<BT.G.Reference>>, domain: set<BT.G.Reference>)
    decreases graph, domain
  {
    forall ref: NativeTypes.uint64 {:trigger graph[ref]} {:trigger ref in domain} | ref in graph && ref in domain :: 
      forall i: int {:trigger graph[ref][i]} | 0 <= i < |graph[ref]| :: 
        graph[ref][i] in graph
  }
  predicate GraphClosedRestrictedPartial(graph: map<BT.G.Reference, seq<BT.G.Reference>>, domain: set<BT.G.Reference>, next: BT.G.Reference, j: int)
    requires next in graph
    requires 0 <= j <= |graph[next]|
    decreases graph, domain, next, j
  {
    GraphClosedRestricted(graph, domain) &&
    forall i: int {:trigger graph[next][i]} | 0 <= i < j :: 
      graph[next][i] in graph
  }
  function IsRoot(ref: BT.G.Reference): int
    decreases ref
  {
    if ref == BT.G.Root() then
      1
    else
      0
  }
  predicate ValidPredCounts(predCounts: map<BT.G.Reference, int>, graph: map<BT.G.Reference, seq<BT.G.Reference>>)
    decreases predCounts, graph
  {
    forall ref: NativeTypes.uint64 {:trigger IsRoot(ref)} {:trigger PredecessorSet(graph, ref)} {:trigger predCounts[ref]} {:trigger ref in predCounts} | ref in predCounts :: 
      predCounts[ref] == |PredecessorSet(graph, ref)| + IsRoot(ref)
  }
  function MaxSize(): int
  {
    IndirectionTableMaxSize()
  }
  function method MaxSizeUint64(): uint64
  {
    IndirectionTableMaxSizeUint64()
  }
}
Dafny program verifier did not attempt verification